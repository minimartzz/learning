{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 22/10/2025   | Martin | Created   | Notebook to learn about backpropagation | \n",
    "| 24/10/2025   | Martin | Update   | Started with individual element derivatives | \n",
    "| 27/10/2025   | Martin | Update   | Continued with backpropagation | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Dataset Creation](#dataset-creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation\n",
    "\n",
    "Same functions as previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in all the words\n",
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {v: k+1 for k, v in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {v: k for k, v in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "  block_size = 3\n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix]\n",
    "    \n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "X_train, y_train = build_dataset(words[:n1])\n",
    "X_val, y_val = build_dataset(words[n1:n2])\n",
    "X_test, y_test = build_dataset(words[n2:])\n",
    "\n",
    "block_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Backpropogation\n",
    "\n",
    "Backpropogating through all of the variables as they are defined in the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare the gradients between manually calculated and torch calculated\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item() # Checks if they are exactly the same\n",
    "  app = torch.allclose(dt, t.grad) # Checks if they are within a tolerance\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f\"{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation of the weights and bias is slightly different by multiplying some small value to each of them. This is because soemtimes initialising with e.g all zeros could mask incorrect implementation of the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 4137\n"
     ]
    }
   ],
   "source": [
    "# Initialise weights and biases\n",
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g) \n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size) ** 0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.ones((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.ones((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "print(f\"Total number of parameters: {sum(p.nelement() for p in parameters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size\n",
    "\n",
    "# Minibatch\n",
    "ix = torch.randint(0, X_train.shape[0], (batch_size, ), generator=g)\n",
    "X_batch, y_batch = X_train[ix], y_train[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3603, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explicit forward pass\n",
    "emb = C[X_batch]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "# Linear layer 1\n",
    "h_pre_bn = embcat @ W1 + b1 # Hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bn_mean_i = 1/n * h_pre_bn.sum(0, keepdim=True)\n",
    "bn_diff = h_pre_bn - bn_mean_i\n",
    "bn_diff_sq = bn_diff ** 2\n",
    "bn_var = 1/(n-1) * (bn_diff_sq).sum(0, keepdim=True)\n",
    "bn_var_inv = (bn_var + 1e-5)**-0.5\n",
    "bn_raw = bn_diff * bn_var_inv\n",
    "h_preact = bngain * bn_raw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(h_preact)\n",
    "\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2\n",
    "# Cross entropy loss\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), y_batch].mean()\n",
    "\n",
    "# Pytorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [\n",
    "  logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "  norm_logits, logit_maxes, logits, h, h_preact, bn_raw,\n",
    "  bn_var, bn_var_inv, bn_diff_sq, bn_diff, h_pre_bn, bn_mean_i,\n",
    "  embcat, emb\n",
    "]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropogation here\n",
    "\n",
    "- Sizes of the derivatives are always the same as their original tensors: Use the size of the tensors to figure out what to do\n",
    "- From Math: Derivatives (gradients) will always sum their components\n",
    "\n",
    "<u>Notes of Interpretation</u>\n",
    "\n",
    "- `probs`: If the probability of the correct class is low, it's boosting the derivative of the log probs to adjust the weights for the correct class\n",
    "- `norm_logits`: Gradient of `logit_maxes` should be zero (or close to due to floating point precision). This is because it only scales the values to prevent overflow during the exponent in the subsequent step. Since the output is a softmax, the probabilities don't change, therefore, it should not have any impact on the update step i.e 0 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "count_sum_inv   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h_preact        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_raw          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_var_inv      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_var          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_diff_sq      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_diff         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h_pre_bn        | exact: False | approximate: False | maxdiff: 0.001161645632237196\n",
      "bn_mean_i       | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "d_logprobs = torch.zeros_like(logprobs)\n",
    "d_logprobs[range(n), y_batch] = -1.0/n # derivative\n",
    "cmp('logprobs', d_logprobs, logprobs)\n",
    "\n",
    "d_probs = (1.0 / probs) * d_logprobs # chain rule \n",
    "cmp('probs', d_probs, probs)\n",
    "\n",
    "d_counts_sum_inv = (counts * d_probs).sum(1, keepdim=True)\n",
    "cmp('count_sum_inv', d_counts_sum_inv, counts_sum_inv)\n",
    "\n",
    "d_counts_sum = (-1.0 / counts_sum**2.0) * d_counts_sum_inv\n",
    "cmp('counts_sum', d_counts_sum, counts_sum)\n",
    "\n",
    "# d_counts is being used twice\n",
    "# 1. probs = counts * counts_sum_inv\n",
    "# 2. counts_sum = counts.sum(...)\n",
    "d_counts = d_probs * counts_sum_inv + torch.ones_like(counts) * d_counts_sum\n",
    "cmp('counts', d_counts, counts)\n",
    "\n",
    "d_norm_logits = norm_logits.exp() * d_counts\n",
    "cmp('norm_logits', d_norm_logits, norm_logits)\n",
    "\n",
    "d_logit_maxes = (-d_norm_logits).sum(1, keepdim=True)\n",
    "cmp('logit_maxes', d_logit_maxes, logit_maxes)\n",
    "\n",
    "# 1. norm_logits\n",
    "# 2. logit_maxes\n",
    "temp = F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * d_logit_maxes # logits_max\n",
    "d_logits = (d_norm_logits.clone()) + temp\n",
    "cmp('logits', d_logits, logits)\n",
    "\n",
    "# Linear layer\n",
    "d_h = d_logits @ W2.T\n",
    "d_W2 = h.T @ d_logits\n",
    "d_b2 = d_logits = d_logits.sum(0)\n",
    "cmp('h', d_h, h)\n",
    "cmp('W2', d_W2, W2)\n",
    "cmp('b2', d_b2, b2)\n",
    "\n",
    "# Non-linearity\n",
    "d_h_preact = (1.0 - h**2)  * d_h\n",
    "cmp('h_preact', d_h_preact, h_preact)\n",
    "\n",
    "# Batch Normalisation\n",
    "d_bngain = (bn_raw * d_h_preact).sum(0, keepdim=True)\n",
    "d_bn_raw = bngain * d_h_preact\n",
    "d_bnbias = d_h_preact.sum(0, keepdim=True)\n",
    "cmp('bngain', d_bngain, bngain)\n",
    "cmp('bn_raw', d_bn_raw, bn_raw)\n",
    "cmp('bnbias', d_bnbias, bnbias)\n",
    "\n",
    "d_bn_var_inv = (bn_diff * d_bn_raw).sum(0, keepdim=True)\n",
    "cmp('bn_var_inv', d_bn_var_inv, bn_var_inv)\n",
    "\n",
    "# d_bn_var = (-0.5) * (1.0/(bn_var + 1e-5)**(3/2)) * d_bn_var_inv\n",
    "d_bn_var = (-0.5*(bn_var + 1e-5)**-1.5) * d_bn_var_inv\n",
    "cmp('bn_var', d_bn_var, bn_var)\n",
    "\n",
    "d_bn_diff_sq = (1.0/(n-1)) * torch.ones_like(bn_diff_sq) * d_bn_var\n",
    "cmp('bn_diff_sq', d_bn_diff_sq, bn_diff_sq)\n",
    "\n",
    "# 1. bn_raw = bn_diff * bn_var_inv\n",
    "# 2. dn_diff_sq = bn_diff**2\n",
    "temp = 2 * bn_diff * d_bn_diff_sq\n",
    "d_bn_diff = bn_var_inv * d_bn_raw + temp\n",
    "cmp('bn_diff', d_bn_diff, bn_diff)\n",
    "\n",
    "d_bn_mean_i = (-d_bn_diff).sum(0)\n",
    "cmp('bn_mean_i', d_bn_mean_i, bn_mean_i)\n",
    "\n",
    "# 1. bn_diff = h_pre_bn - bn_mean_i\n",
    "d_h_pre_bn = d_bn_diff.clone()\n",
    "cmp('h_pre_bn', d_h_pre_bn, h_pre_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_mean_i.shape, d_bn_diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
