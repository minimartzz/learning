{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "[Building makemore pt 5: Building a WaveNet](https://www.youtube.com/watch?v=t3YJ5hKiMQ0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 22/10/2025   | Martin | Created   | Notebook to learn about backpropagation | \n",
    "| 24/10/2025   | Martin | Update   | Started with individual element derivatives | \n",
    "| 27/10/2025   | Martin | Update   | Continued with backpropagation | \n",
    "| 28/10/2025   | Martin | Update   | Completed backpropagation | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Dataset Creation](#dataset-creation)\n",
    "* [Manual Backpropagation](#manual-backpropogation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation\n",
    "\n",
    "Same functions as previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in all the words\n",
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {v: k+1 for k, v in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {v: k for k, v in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "  block_size = 3\n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix]\n",
    "    \n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "X_train, y_train = build_dataset(words[:n1])\n",
    "X_val, y_val = build_dataset(words[n1:n2])\n",
    "X_test, y_test = build_dataset(words[n2:])\n",
    "\n",
    "block_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Backpropogation\n",
    "\n",
    "Backpropogating through all of the variables as they are defined in the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare the gradients between manually calculated and torch calculated\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item() # Checks if they are exactly the same\n",
    "  app = torch.allclose(dt, t.grad) # Checks if they are within a tolerance\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f\"{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation of the weights and bias is slightly different by multiplying some small value to each of them. This is because soemtimes initialising with e.g all zeros could mask incorrect implementation of the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 4137\n"
     ]
    }
   ],
   "source": [
    "# Initialise weights and biases\n",
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g) \n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size) ** 0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.ones((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.ones((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "print(f\"Total number of parameters: {sum(p.nelement() for p in parameters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size\n",
    "\n",
    "# Minibatch\n",
    "ix = torch.randint(0, X_train.shape[0], (batch_size, ), generator=g)\n",
    "X_batch, y_batch = X_train[ix], y_train[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3603, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explicit forward pass\n",
    "emb = C[X_batch]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "# Linear layer 1\n",
    "h_pre_bn = embcat @ W1 + b1 # Hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bn_mean_i = 1/n * h_pre_bn.sum(0, keepdim=True)\n",
    "bn_diff = h_pre_bn - bn_mean_i\n",
    "bn_diff_sq = bn_diff ** 2\n",
    "bn_var = 1/(n-1) * (bn_diff_sq).sum(0, keepdim=True)\n",
    "bn_var_inv = (bn_var + 1e-5)**-0.5\n",
    "bn_raw = bn_diff * bn_var_inv\n",
    "h_preact = bngain * bn_raw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(h_preact)\n",
    "\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2\n",
    "# Cross entropy loss\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), y_batch].mean()\n",
    "\n",
    "# Pytorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [\n",
    "  logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "  norm_logits, logit_maxes, logits, h, h_preact, bn_raw,\n",
    "  bn_var, bn_var_inv, bn_diff_sq, bn_diff, h_pre_bn, bn_mean_i,\n",
    "  embcat, emb\n",
    "]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropogation here\n",
    "\n",
    "- Sizes of the derivatives are always the same as their original tensors: Use the size of the tensors to figure out what to do\n",
    "- From Math: Derivatives (gradients) will always sum their components\n",
    "\n",
    "<u>Notes of Interpretation</u>\n",
    "\n",
    "- `probs`: If the probability of the correct class is low, it's boosting the derivative of the log probs to adjust the weights for the correct class\n",
    "- `norm_logits`: Gradient of `logit_maxes` should be zero (or close to due to floating point precision). This is because it only scales the values to prevent overflow during the exponent in the subsequent step. Since the output is a softmax, the probabilities don't change, therefore, it should not have any impact on the update step i.e 0 gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Matrix Derivatives\n",
    "\n",
    "![image](./assets/matrix_derivative.png)\n",
    "![image](./assets/matrix_derivative_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "count_sum_inv   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h_preact        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_raw          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_var_inv      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_var          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_diff_sq      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_diff         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bn_mean_i       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h_pre_bn        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "d_logprobs = torch.zeros_like(logprobs)\n",
    "d_logprobs[range(n), y_batch] = -1.0/n # derivative\n",
    "cmp('logprobs', d_logprobs, logprobs)\n",
    "\n",
    "d_probs = (1.0 / probs) * d_logprobs # chain rule \n",
    "cmp('probs', d_probs, probs)\n",
    "\n",
    "d_counts_sum_inv = (counts * d_probs).sum(1, keepdim=True)\n",
    "cmp('count_sum_inv', d_counts_sum_inv, counts_sum_inv)\n",
    "\n",
    "d_counts_sum = (-1.0 / counts_sum**2.0) * d_counts_sum_inv\n",
    "cmp('counts_sum', d_counts_sum, counts_sum)\n",
    "\n",
    "# d_counts is being used twice\n",
    "# 1. probs = counts * counts_sum_inv\n",
    "# 2. counts_sum = counts.sum(...)\n",
    "d_counts = d_probs * counts_sum_inv + torch.ones_like(counts) * d_counts_sum\n",
    "cmp('counts', d_counts, counts)\n",
    "\n",
    "d_norm_logits = norm_logits.exp() * d_counts\n",
    "cmp('norm_logits', d_norm_logits, norm_logits)\n",
    "\n",
    "d_logit_maxes = (-d_norm_logits).sum(1, keepdim=True)\n",
    "cmp('logit_maxes', d_logit_maxes, logit_maxes)\n",
    "\n",
    "# 1. norm_logits\n",
    "# 2. logit_maxes\n",
    "temp = F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * d_logit_maxes # logits_max\n",
    "d_logits = (d_norm_logits.clone()) + temp\n",
    "cmp('logits', d_logits, logits)\n",
    "\n",
    "\n",
    "# Linear layer 2\n",
    "d_h = d_logits @ W2.T\n",
    "d_W2 = h.T @ d_logits\n",
    "d_b2 = d_logits = d_logits.sum(0)\n",
    "cmp('h', d_h, h)\n",
    "cmp('W2', d_W2, W2)\n",
    "cmp('b2', d_b2, b2)\n",
    "\n",
    "\n",
    "# Non-linearity\n",
    "d_h_preact = (1.0 - h**2)  * d_h\n",
    "cmp('h_preact', d_h_preact, h_preact)\n",
    "\n",
    "\n",
    "# Batch Normalisation\n",
    "d_bngain = (bn_raw * d_h_preact).sum(0, keepdim=True)\n",
    "d_bn_raw = bngain * d_h_preact\n",
    "d_bnbias = d_h_preact.sum(0, keepdim=True)\n",
    "cmp('bngain', d_bngain, bngain)\n",
    "cmp('bn_raw', d_bn_raw, bn_raw)\n",
    "cmp('bnbias', d_bnbias, bnbias)\n",
    "\n",
    "d_bn_var_inv = (bn_diff * d_bn_raw).sum(0, keepdim=True)\n",
    "cmp('bn_var_inv', d_bn_var_inv, bn_var_inv)\n",
    "\n",
    "# d_bn_var = (-0.5) * (1.0/(bn_var + 1e-5)**(3/2)) * d_bn_var_inv\n",
    "d_bn_var = (-0.5*(bn_var + 1e-5)**-1.5) * d_bn_var_inv\n",
    "cmp('bn_var', d_bn_var, bn_var)\n",
    "\n",
    "d_bn_diff_sq = (1.0/(n-1)) * torch.ones_like(bn_diff_sq) * d_bn_var\n",
    "cmp('bn_diff_sq', d_bn_diff_sq, bn_diff_sq)\n",
    "\n",
    "# 1. bn_raw = bn_diff * bn_var_inv\n",
    "# 2. dn_diff_sq = bn_diff**2\n",
    "temp = 2 * bn_diff * d_bn_diff_sq\n",
    "d_bn_diff = bn_var_inv * d_bn_raw + temp\n",
    "cmp('bn_diff', d_bn_diff, bn_diff)\n",
    "\n",
    "d_bn_mean_i = (-d_bn_diff).sum(0)\n",
    "cmp('bn_mean_i', d_bn_mean_i, bn_mean_i)\n",
    "\n",
    "# 1. bn_diff = h_pre_bn - bn_mean_i\n",
    "# 2. bn_mean_i = 1/n * h_pre_bn.sum(0, keepdim=True)\n",
    "d_h_pre_bn = d_bn_diff.clone() + torch.ones_like(h_pre_bn) * 1.0/n * d_bn_mean_i\n",
    "cmp('h_pre_bn', d_h_pre_bn, h_pre_bn)\n",
    "\n",
    "\n",
    "# Linear Layer 1\n",
    "d_embcat = d_h_pre_bn @ W1.T\n",
    "d_W1 = embcat.T @ d_h_pre_bn\n",
    "d_b1 = d_h_pre_bn.sum(0)\n",
    "cmp('embcat', d_embcat, embcat)\n",
    "cmp('W1', d_W1, W1)\n",
    "cmp('b1', d_b1, b1)\n",
    "\n",
    "\n",
    "# Embeddings - re-represent the shape\n",
    "d_emb = d_embcat.view(emb.shape)\n",
    "cmp('emb', d_emb, emb)\n",
    "\n",
    "d_C = torch.zeros_like(C)\n",
    "for i in range(X_batch.shape[0]):\n",
    "  for j in range(X_batch.shape[1]):\n",
    "    idx = X_batch[i, j]\n",
    "    d_C[idx] += d_emb[i, j]\n",
    "cmp('C', d_C, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation on Combined Expressions\n",
    "\n",
    "Instead of breaking formulas up into individual pieces, the combined computation when differentiated can be simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Using Cross Entropy to compute the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.36032772064209 diff: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "loss_fast = F.cross_entropy(logits, y_batch)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivation of the the derivative of cross entropy\n",
    "\n",
    "![image](./assets/cross_entropy_derivative.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.05359673500061e-09\n"
     ]
    }
   ],
   "source": [
    "# Backward pass for Softmax\n",
    "d_logits = F.softmax(logits, 1)\n",
    "d_logits[range(n), y_batch] -= 1\n",
    "d_logits /= n\n",
    "\n",
    "cmp('logits', d_logits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivation of derivative for Batch Normalisation\n",
    "\n",
    "![image](./assets/batch_normalisation_derivative.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_pre_bn        | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# Not easy to get this formula, because the formula above only does it for one neuron\n",
    "# But there are 64 neurons that must all be done in parallel to ensure the shapes match\n",
    "d_h_pre_bn = bngain * bn_var_inv/n * (n*d_h_preact - d_h_preact.sum(0) - n/(n-1)*bn_raw*(d_h_preact*bn_raw).sum(0))\n",
    "cmp('h_pre_bn', d_h_pre_bn, h_pre_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 4137\n",
      "      0/ 200000: 3.3603\n",
      "  10000/ 200000: 2.4803\n",
      "  20000/ 200000: 2.1109\n",
      "  30000/ 200000: 2.3620\n",
      "  40000/ 200000: 2.3571\n",
      "  50000/ 200000: 2.2323\n",
      "  60000/ 200000: 2.2413\n",
      "  70000/ 200000: 2.4117\n",
      "  80000/ 200000: 2.1686\n",
      "  90000/ 200000: 2.4417\n",
      " 100000/ 200000: 2.1538\n",
      " 110000/ 200000: 1.9201\n",
      " 120000/ 200000: 2.0154\n",
      " 130000/ 200000: 2.6449\n",
      " 140000/ 200000: 1.9107\n",
      " 150000/ 200000: 2.2465\n",
      " 160000/ 200000: 2.1470\n",
      " 170000/ 200000: 2.2498\n",
      " 180000/ 200000: 2.0757\n",
      " 190000/ 200000: 2.0521\n"
     ]
    }
   ],
   "source": [
    "# Initialise weights and biases\n",
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g) \n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size) ** 0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.ones((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.ones((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "print(f\"Total number of parameters: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "# Model\n",
    "max_steps = 200_000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  # minibatch\n",
    "  ix = torch.randint(0, X_train.shape[0], (batch_size,), generator=g)\n",
    "  X_batch, y_batch = X_train[ix], y_train[ix]\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[X_batch]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 # Bias was removed\n",
    "  bnmeani = hpreact.mean(0, keepdim=True)\n",
    "  bnstdi = hpreact.std(0, keepdim=True)\n",
    "\n",
    "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "  h = torch.tanh(hpreact)\n",
    "  logits = h @ W2 + b2\n",
    "  loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # ---------- Manual Backprop ----------\n",
    "  dlogits = F.softmax(logits, 1)\n",
    "  dlogits[range(n), y_batch] -= 1\n",
    "  dlogits /= n\n",
    "  # 2nd layer backprop\n",
    "  dh = dlogits @ W2.T\n",
    "  dW2 = h.T @ dlogits\n",
    "  db2 = dlogits.sum(0)\n",
    "  # tanh\n",
    "  dhpreact = (1.0 - h**2) * dh\n",
    "  # batchnorm backprop\n",
    "  dbngain = (bn_raw * dhpreact).sum(0, keepdim=True)\n",
    "  dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "  dhprebn = bngain*bn_var_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bn_raw*(dhpreact*bn_raw).sum(0))\n",
    "  # 1st layer\n",
    "  dembcat = dhprebn @ W1.T\n",
    "  dW1 = embcat.T @ dhprebn\n",
    "  db1 = dhprebn.sum(0)\n",
    "  # embedding\n",
    "  demb = dembcat.view(emb.shape)\n",
    "  dC = torch.zeros_like(C)\n",
    "  for k in range(X_batch.shape[0]):\n",
    "    for j in range(X_batch.shape[1]):\n",
    "      ix = X_batch[k,j]\n",
    "      dC[ix] += demb[k,j]\n",
    "  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias] \n",
    "  # -------------------------------------\n",
    "\n",
    "  # Update\n",
    "  lr = 0.1 if i < 100_000 else 0.01\n",
    "  for p, grad in zip(parameters, grads):\n",
    "    p.data += -lr * grad\n",
    "    # p.data += -lr * p.grad\n",
    "  \n",
    "  # Track stats\n",
    "  if i % 10_000 == 0:\n",
    "    print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "  lossi.append(loss.log10().item())\n",
    "\n",
    "  # if i >= 100:\n",
    "  #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  emb = C[X_train]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 7.0931854248046875\n",
      "val 7.030697345733643\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "  x, y = {\n",
    "    'train': (X_train, y_train),\n",
    "    'val': (X_val, y_val),\n",
    "    'test': (X_test, y_test)\n",
    "  }[split]\n",
    "\n",
    "  emb = C[x]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact)\n",
    "  logits = h @ W2 + b2\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
