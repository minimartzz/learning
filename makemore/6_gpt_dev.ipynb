{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building GPT From Scratch\n",
    "\n",
    "[Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=11s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 05/11/2025   | Martin | Created   | Notebook created to build GPT from scratch | \n",
    "| 10/11/2025   | Martin | Update  | Working on self-attention mechanism | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Load Data](#load-data)\n",
    "* [Bigram Model](#bigram-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Using the Tiny Shakespeare dataset found [here](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt).\n",
    "\n",
    "Training a model on Shakespeare's text to predict the next character given a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in chraceters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of dataset in chraceters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First thousand characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Number of unique chracters: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(f\"Number of unique chracters: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappers \n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda x: [stoi[c] for c in x] # Convert strings to index\n",
    "decode = lambda x: ''.join([itos[c] for c in x]) # Convert index to strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert entire text into indexes, then split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = 0.9\n",
    "idx = int(sp * len(data))\n",
    "train_data = data[:idx]\n",
    "val_data = data[idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sequence of letters are training data is actually multiple training instances. The context length grows from the shortest length predicting every next letter till the full sequence of letters is formed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]), the target is 47\n",
      "When input is tensor([18, 47]), the target is 56\n",
      "When input is tensor([18, 47, 56]), the target is 57\n",
      "When input is tensor([18, 47, 56, 57]), the target is 58\n",
      "When input is tensor([18, 47, 56, 57, 58]), the target is 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), the target is 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train_data[:8]\n",
    "y = train_data[1:8+1]\n",
    "for t in range(block_size):\n",
    "  inp = x[:t+1]\n",
    "  out = y[t]\n",
    "  print(f\"When input is {inp}, the target is {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "outputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "When input is [24], the target is 43\n",
      "When input is [24, 43], the target is 58\n",
      "When input is [24, 43, 58], the target is 5\n",
      "When input is [24, 43, 58, 5], the target is 57\n",
      "When input is [24, 43, 58, 5, 57], the target is 1\n",
      "When input is [24, 43, 58, 5, 57, 1], the target is 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46], the target is 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43], the target is 39\n",
      "\n",
      "When input is [44], the target is 53\n",
      "When input is [44, 53], the target is 56\n",
      "When input is [44, 53, 56], the target is 1\n",
      "When input is [44, 53, 56, 1], the target is 58\n",
      "When input is [44, 53, 56, 1, 58], the target is 46\n",
      "When input is [44, 53, 56, 1, 58, 46], the target is 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39], the target is 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58], the target is 1\n",
      "\n",
      "When input is [52], the target is 58\n",
      "When input is [52, 58], the target is 1\n",
      "When input is [52, 58, 1], the target is 58\n",
      "When input is [52, 58, 1, 58], the target is 46\n",
      "When input is [52, 58, 1, 58, 46], the target is 39\n",
      "When input is [52, 58, 1, 58, 46, 39], the target is 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58], the target is 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1], the target is 46\n",
      "\n",
      "When input is [25], the target is 17\n",
      "When input is [25, 17], the target is 27\n",
      "When input is [25, 17, 27], the target is 10\n",
      "When input is [25, 17, 27, 10], the target is 0\n",
      "When input is [25, 17, 27, 10, 0], the target is 21\n",
      "When input is [25, 17, 27, 10, 0, 21], the target is 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1], the target is 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54], the target is 39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating minibatch\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "  x = torch.stack([train_data[i:i+block_size] for i in idx])\n",
    "  y = torch.stack([train_data[i+1:i+1+block_size] for i in idx])\n",
    "  return x, y\n",
    "\n",
    "X_batch, y_batch = get_batch(\"train\")\n",
    "print(\"inputs:\")\n",
    "print(X_batch.shape)\n",
    "print(X_batch)\n",
    "print(\"outputs:\")\n",
    "print(y_batch.shape)\n",
    "print(y_batch)\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "  for t in range(block_size):\n",
    "    context = X_batch[b, :t+1]\n",
    "    target = y_batch[b, t]\n",
    "    print(f\"When input is {context.tolist()}, the target is {target}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x29ba91b5330>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    # Each token directly reads off the logits for the next token from a lookup table\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # B - batch size\n",
    "    # T - time (similar to tracking which set of text has been seen)\n",
    "    # C - embedding size\n",
    "    logits = self.token_embedding_table(idx)\n",
    "\n",
    "    # Reshape the logits tensor to match those used by cross entropy\n",
    "    if targets == None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      # get the predictions\n",
    "      logits, loss = self(idx)\n",
    "      # focus on the last time step\n",
    "      logits = logits[:, -1, :]\n",
    "      # apply softmax to get probabilities\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      idx_next = torch.multinomial(probs, num_samples=1)\n",
    "      # append the sampled index to the existing sequence\n",
    "      idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(X_batch, y_batch)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "starting_idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(starting_idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pytorch optimiser (usually for Adam, lr=1e-4 to 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimiser\n",
    "optimiser = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5319576263427734\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "batch_size = 32\n",
    "for _ in range(100_000):\n",
    "  X_batch, y_batch = get_batch('train')\n",
    "\n",
    "  optimiser.zero_grad(set_to_none=True)\n",
    "  logits, loss = m(X_batch, y_batch)\n",
    "\n",
    "  loss.backward()\n",
    "  optimiser.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ofows ht IUS:\n",
      "S:\n",
      "\n",
      "ING flvenje ssutefr,\n",
      "M:\n",
      "War cl igagimous pray whars:\n",
      "Panalit I It aithit terised \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(starting_idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "The _self-attention_ mechanism is where the current token uses information from previous tokens to predict the current token. There are many variations but the most famous one was introduced in the Transformer architecture. Here we step through the different approches in increasing complexity\n",
    "\n",
    "1. Iteratively find weights by looping\n",
    "2. Matrix multiplication to aggregate\n",
    "3. Softmax\n",
    "4. Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Iteratively find through elementwise looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inefficient method of computing average (elementwise)\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "  for t in range(T):\n",
    "    xprev = x[b, :t+1]\n",
    "    xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "----------\n",
      "b=\n",
      "tensor([[8., 8.],\n",
      "        [5., 7.],\n",
      "        [5., 0.]])\n",
      "----------\n",
      "sum=\n",
      "tensor([[ 8.,  8.],\n",
      "        [13., 15.],\n",
      "        [18., 15.]])\n",
      "Above shows that using a bottom triangle matrix can get the sum of the weights of previous tokens\n",
      "----------\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "mean=\n",
      "tensor([[8.0000, 8.0000],\n",
      "        [6.5000, 7.5000],\n",
      "        [6.0000, 5.0000]])\n",
      "Here is the average\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Showcasing the matrix methods is more efficient\n",
    "torch.manual_seed(43)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"----------\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"----------\")\n",
    "print(\"sum=\")\n",
    "print(a @ b)\n",
    "print(\"Above shows that using a bottom triangle matrix can get the sum of \"\n",
    "      \"the weights of previous tokens\")\n",
    "print(\"----------\")\n",
    "\n",
    "c = a / torch.sum(a, 1, keepdims=True)\n",
    "print(c)\n",
    "print(\"mean=\")\n",
    "print(c @ b)\n",
    "print(\"Here is the average\")\n",
    "print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using matrix multiplication\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdims=True)\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) ---> (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using softmax to create the original weight matrix\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow3, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Self-attention\n",
    "\n",
    "- `key` - A representation of all the existing tokens that came before\n",
    "- `query` - A request for information from other tokens from the current token\n",
    "- `key @ query` - Is like a question-answer system: \"What tokens are relevant to the current token?\" -> If the output is high, then that token's weights will be transferred in this current batch\n",
    "- `value` - The value matrix is \"universal\" transformation layer for any embedding batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Single Head of self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-1, -2) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Notes on Attention</u>\n",
    "\n",
    "- Attention is a __communication mecahnism__. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights\n",
    "- There is no notion of space between the words, Attention acts over a set of vectors => Why we need positional encodings\n",
    "- Each example across the batch dimension is processed independently and never \"talk\" to each other\n",
    "- In this implementation, we mask the subsequent tokens from the token in question because we are performing a text generation task, so tokens are not allowed to observe subsequent values. However, in other tasks e.g sentiment analysis, this constraint can be removed\n",
    "  - Remove masking done with `tril` will allow all tokens to communicate. The block is called the decoder attention block because it has triangular masking, and is usually used in autoregressive setting e.g language modeling\n",
    "- _Self-attention_ means the `keys` and `values` are prodcued from the same source as queries. In _Cross-attention_ the queries are produced from the original data, but the `keys` and `values` come from some external source\n",
    "- Scaled attention additional divides the `wei` matrix by $1/\\sqrt{(head_size)}$. This makes it so what input $Q$, $K$ are unit variance, `wei` will be unit variance too and Softmax will stay diffuse and not saturate that much\n",
    "  - See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax in the second cell has a much sharper distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
