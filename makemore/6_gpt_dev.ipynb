{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building GPT From Scratch\n",
    "\n",
    "[Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=11s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 05/11/2025   | Martin | Created   | Notebook created to build GPT from scratch | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Load Data](#load-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Using the Tiny Shakespeare dataset found [here](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt).\n",
    "\n",
    "Training a model on Shakespeare's text to predict the next character given a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in chraceters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of dataset in chraceters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First thousand characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Number of unique chracters: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(f\"Number of unique chracters: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappers \n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda x: [stoi[c] for c in x] # Convert strings to index\n",
    "decode = lambda x: ''.join([itos[c] for c in x]) # Convert index to strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert entire text into indexes, then split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = 0.9\n",
    "idx = int(sp * len(data))\n",
    "train_data = data[:idx]\n",
    "val_data = data[idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sequence of letters are training data is actually multiple training instances. The context length grows from the shortest length predicting every next letter till the full sequence of letters is formed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]), the target is 47\n",
      "When input is tensor([18, 47]), the target is 56\n",
      "When input is tensor([18, 47, 56]), the target is 57\n",
      "When input is tensor([18, 47, 56, 57]), the target is 58\n",
      "When input is tensor([18, 47, 56, 57, 58]), the target is 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), the target is 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train_data[:8]\n",
    "y = train_data[1:8+1]\n",
    "for t in range(block_size):\n",
    "  inp = x[:t+1]\n",
    "  out = y[t]\n",
    "  print(f\"When input is {inp}, the target is {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "outputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "When input is [24], the target is 43\n",
      "When input is [24, 43], the target is 58\n",
      "When input is [24, 43, 58], the target is 5\n",
      "When input is [24, 43, 58, 5], the target is 57\n",
      "When input is [24, 43, 58, 5, 57], the target is 1\n",
      "When input is [24, 43, 58, 5, 57, 1], the target is 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46], the target is 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43], the target is 39\n",
      "\n",
      "When input is [44], the target is 53\n",
      "When input is [44, 53], the target is 56\n",
      "When input is [44, 53, 56], the target is 1\n",
      "When input is [44, 53, 56, 1], the target is 58\n",
      "When input is [44, 53, 56, 1, 58], the target is 46\n",
      "When input is [44, 53, 56, 1, 58, 46], the target is 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39], the target is 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58], the target is 1\n",
      "\n",
      "When input is [52], the target is 58\n",
      "When input is [52, 58], the target is 1\n",
      "When input is [52, 58, 1], the target is 58\n",
      "When input is [52, 58, 1, 58], the target is 46\n",
      "When input is [52, 58, 1, 58, 46], the target is 39\n",
      "When input is [52, 58, 1, 58, 46, 39], the target is 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58], the target is 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1], the target is 46\n",
      "\n",
      "When input is [25], the target is 17\n",
      "When input is [25, 17], the target is 27\n",
      "When input is [25, 17, 27], the target is 10\n",
      "When input is [25, 17, 27, 10], the target is 0\n",
      "When input is [25, 17, 27, 10, 0], the target is 21\n",
      "When input is [25, 17, 27, 10, 0, 21], the target is 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1], the target is 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54], the target is 39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating minibatch\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "  x = torch.stack([train_data[i:i+block_size] for i in idx])\n",
    "  y = torch.stack([train_data[i+1:i+1+block_size] for i in idx])\n",
    "  return x, y\n",
    "\n",
    "X_batch, y_batch = get_batch(\"train\")\n",
    "print(\"inputs:\")\n",
    "print(X_batch.shape)\n",
    "print(X_batch)\n",
    "print(\"outputs:\")\n",
    "print(y_batch.shape)\n",
    "print(y_batch)\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "  for t in range(block_size):\n",
    "    context = X_batch[b, :t+1]\n",
    "    target = y_batch[b, t]\n",
    "    print(f\"When input is {context.tolist()}, the target is {target}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19ed3e7cdf0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    # Each token directly reads off the logits for the next token from a lookup table\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # B - batch size\n",
    "    # T - time (similar to tracking which set of text has been seen)\n",
    "    # C - embedding size\n",
    "    logits = self.token_embedding_table(idx)\n",
    "\n",
    "    # Reshape the logits tensor to match those used by cross entropy\n",
    "    if targets == None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      # get the predictions\n",
    "      logits, loss = self(idx)\n",
    "      # focus on the last time step\n",
    "      logits = logits[:, -1, :]\n",
    "      # apply softmax to get probabilities\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      idx_next = torch.multinomial(probs, num_samples=1)\n",
    "      # append the sampled index to the existing sequence\n",
    "      idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8380, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(X_batch, y_batch)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C'DOKwZrFmsRw?VlCzbkKFlrchHi.zvYdwI.UcMjF&TmtHXUxP\n",
      "AYJ'&:;z,TpuuN&3V\n",
      "kHC,BaT&o.JVbmwLEMqkpROwUTBCFH3\n"
     ]
    }
   ],
   "source": [
    "starting_idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(starting_idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pytorch optimiser (usually for Adam, lr=1e-4 to 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimiser\n",
    "optimiser = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.505732297897339\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "batch_size = 32\n",
    "for _ in range(100000):\n",
    "  X_batch, y_batch = get_batch('train')\n",
    "\n",
    "  optimiser.zero_grad(set_to_none=True)\n",
    "  logits, loss = m(X_batch, y_batch)\n",
    "\n",
    "  loss.backward()\n",
    "  optimiser.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Thens, he avel dsicoy bint Tecammearof ake? Shey be e hsat D himalintilfede e:\n",
      "Wine ge ry.\n",
      "ithe fea\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(starting_idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
