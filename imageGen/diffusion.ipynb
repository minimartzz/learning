{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion\n",
    "\n",
    "Exploring various Diffusion model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 27/02/2026   | Martin | Created   | Writing the DDPM algorithm | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [DDPM Algorithm](#ddpm-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM Algorithm\n",
    "\n",
    "The DDPM algorithm is the one that popularised diffusion models. It aims to predict the noise that needs to be removed from a Gaussian noise image to return back to the original distribution\n",
    "\n",
    "__Forward Noise__\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\alpha_t}x_0 + \\sqrt{1 - \\alpha_t} \\epsilon\n",
    "$$\n",
    "\n",
    "__Reverse Noise__\n",
    "\n",
    "$$\n",
    "E_{q, t}\n",
    "  \\left[ \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t (1 - \\bar{\\alpha_t})} ||\\epsilon - \\epsilon_\\theta (x_t, t)||^2 \\right]\n",
    "$$\n",
    "\n",
    "Where the model is trying to predict the noise term\n",
    "\n",
    "$$\\epsilon_\\theta (x_t, t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import deepinv\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Dataset Config ==========\n",
    "device = \"cuda\"\n",
    "batch_size = 32\n",
    "image_size = 256\n",
    "\n",
    "# ========== Training Config ==========\n",
    "lr = 1e-4\n",
    "epochs = 100\n",
    "# Beta schedule\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "timesteps = 1000\n",
    "betas = torch.linspace(beta_start, beta_end, timesteps, device=device)\n",
    "# Alpha terms\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Data Loader\n",
    "# --------------------------------------------------\n",
    "class ImageFolderDataset(Dataset):\n",
    "  \"\"\"\n",
    "  Loads all images from a folder (and optionally subfolders).\n",
    "  Resizes images to image_size x image_size and normalised to [-1, 1]\n",
    "  \"\"\"\n",
    "  def __init__(\n",
    "    self,\n",
    "    folder: str,\n",
    "    image_size: int = 256,\n",
    "    recursive: bool = True\n",
    "  ):\n",
    "    self.paths = []\n",
    "    folder = Path(folder)\n",
    "    glob = folder.rglob('*') if recursive else folder.glob('*')\n",
    "    for p in glob:\n",
    "      self.paths.append(p)\n",
    "    \n",
    "    if len(self.paths) == 0:\n",
    "      raise ValueError(f\"No images found in {folder}\")\n",
    "    \n",
    "    print(f\"Found {len(self.paths)} images in {folder}\")\n",
    "\n",
    "    self.transform = transforms.Compose([\n",
    "      transforms.Resize(image_size, interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "      transforms.CenterCrop(image_size),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.0, 0.0, 0.0], [1.0, 1.0, 1.0])\n",
    "    ])\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.paths)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    try:\n",
    "      img = Image.open(self.paths[idx]).convert('RGB')\n",
    "      return self.transform(img)\n",
    "    except Exception as e:\n",
    "      print(f\"Warning: failed to load {self.paths[idx]}: {e}. Returning zeros.\")\n",
    "      return torch.zeros(3, self.transform.transforms[1].size, self.transform.transforms[1].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images in data/Celebrity Faces Dataset/Angelina Jolie\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Model Definition\n",
    "# --------------------------------------------------\n",
    "model = deepinv.models.DiffUNet(\n",
    "  in_channels=3,\n",
    "  out_channels=3,\n",
    "  pretrained=None\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "mse = deepinv.loss.MSE()\n",
    "\n",
    "data = ImageFolderDataset(\n",
    "  folder=\"./data/Celebrity Faces Dataset/Angelina Jolie\",\n",
    "  image_size=image_size,\n",
    ")\n",
    "loader = DataLoader(\n",
    "  data,\n",
    "  batch_size=batch_size,\n",
    "  shuffle=True,\n",
    "  num_workers=0,\n",
    "  drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1000) must match the size of tensor b (256) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(imgs)\n\u001b[1;32m      9\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, timesteps, (imgs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), ), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     11\u001b[0m noised_imgs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     12\u001b[0m   sqrt_alphas_cumprod[t, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m imgs\n\u001b[0;32m---> 13\u001b[0m   \u001b[38;5;241m+\u001b[39m \u001b[43msqrt_one_minus_alphas_cumprod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m estimated_noise \u001b[38;5;241m=\u001b[39m model(noised_imgs, t, type_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestep\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1000) must match the size of tensor b (256) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Training Loop\n",
    "# --------------------------------------------------\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  for data in loader:\n",
    "    imgs = data.to(device)\n",
    "    noise = torch.randn_like(imgs)\n",
    "    t = torch.randint(0, timesteps, (imgs.size(0), ), device=device)\n",
    "\n",
    "    noised_imgs = (\n",
    "      sqrt_alphas_cumprod[t, None, None, None] * imgs\n",
    "      + sqrt_one_minus_alphas_cumprod * noise\n",
    "    )\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    estimated_noise = model(noised_imgs, t, type_t='timestep')\n",
    "    loss = mse(estimated_noise, noise)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_ubun_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
