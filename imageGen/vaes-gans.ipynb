{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAEs & GANs\n",
    "\n",
    "Introduction to image generation with VAEs and GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 24/02/2026   | Martin | Created   | VAE model with celeb faces data | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Variational Autoencoders](#variational-autoencoder-vae)\n",
    "* [Generative Adversarial Networks](#generative-adversarial-networks-gans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE)\n",
    "\n",
    "Dataset: [Celeb Faces](https://www.kaggle.com/datasets/vishesh1412/celebrity-face-image-dataset)\n",
    "\n",
    "<u>SOTA Architectures</u>\n",
    "\n",
    "- __VQ-VAE/ VQ-VAE-2__ - Replaces the continuous latent space with discrete codebook. The latent space is quantised to the nearest embedding vector\n",
    "- __Hierarchical VAEs__ - Stacks multiple layers of latent variables. Top latent captures global strucutre, while lower latents capture finer details\n",
    "- __⭐ Latent Diffusion Models__ - VAE compresses image into latent space, then diffusion model operates in that latent space. (Stable Diffusion implementation). VAE is typically a CNN architecture with residual blocks and attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Components ====================\n",
    "class ResBlock(nn.Module):\n",
    "  \"\"\"\n",
    "  Residual block with GroupNorm. GroupNorm >> BatchNorm for VAEs because\n",
    "  batch statistics are unstable with the stochastic latent sampling.\n",
    "  num_groups=32 is standard; reduce if channels < 32\n",
    "  \"\"\"\n",
    "  def __init__(self, in_channels: int, out_channels: int, dropout: float=0.0):\n",
    "    super().__init__()\n",
    "    self.norm1 = nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6)\n",
    "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "    self.norm2 = nn.GroupNorm(num_groups=32, num_channels=out_channels, eps=1e-6)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "    # Residual connection\n",
    "    self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "  def forward(self, x):\n",
    "    h = self.conv1(F.silu(self.norm1(x)))\n",
    "    h = self.conv2(self.dropout(F.silu(self.norm2(h))))\n",
    "    return h + self.skip(x)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "  \"\"\"\n",
    "  Single-head self-attention at spatial positions. Applied at low-resolution\n",
    "  feature maps (e.g., 8x8, 16x16) to keep compute tractable.\n",
    "  Multi-head attention is better but much more expensive for ablations.\n",
    "  \"\"\"\n",
    "  def __init__(self, channels: int):\n",
    "    super().__init__()\n",
    "    self.norm = nn.GroupNorm(32, channels, eps=1e-6)\n",
    "    self.q = nn.Conv2d(channels, channels, 1)\n",
    "    self.k = nn.Conv2d(channels, channels, 1)\n",
    "    self.v = nn.Conv2d(channels, channels, 1)\n",
    "    self.proj = nn.Conv2d(channels, channels, 1)\n",
    "    self.scale = channels ** -0.5\n",
    "  \n",
    "  def forward(self, x):\n",
    "    B, C, H, W = x.shape\n",
    "    h = self.norm(x)\n",
    "    q = self.q(h).reshape(B, C, -1) # (B, C, HW)\n",
    "    k = self.k(h).reshape(B, C, -1)\n",
    "    v = self.v(h).reshape(B, C, -1)\n",
    "    attn = torch.softmax(torch.bmm(q.transpose(1, 2), k) * self.scale, dim=-1) # (B, HW, HW)\n",
    "    out = torch.bmm(v, attn.transpose(1, 2)).reshape(B, C, H, W)\n",
    "\n",
    "    return x + self.proj(out)\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "  \"\"\"Strided conv is better than pooling - learns how to downsample\"\"\"\n",
    "  def __init__(self, channels):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.conv1(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "  \"\"\"Nearest-neighbor + conv avoids checkerboard artifacts from transposed conv.\"\"\"\n",
    "  def __init__(self, channels):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.conv(F.interpolate(x, scale_factor=2, mode='nearest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Encoder ====================\n",
    "class Encoder(nn.Module):\n",
    "  \"\"\"\n",
    "  - channel_multipliers: controls width at each resolution level.\n",
    "    (1, 2, 4, 8) means 4 downsampling stages. More stages = more compression.\n",
    "  - base_channels: 64 for lightweight, 128 for quality, 256 for SOTA.\n",
    "  - latent_dim: 4 is standard for image VAEs (LDM uses 4).\n",
    "    Higher = more expressivity but harder KL regularization.\n",
    "  - attn_resolutions: which spatial sizes get attention. Smaller = more global context.\n",
    "  \"\"\"\n",
    "  def __init__(\n",
    "    self,\n",
    "    in_channels: int=3,\n",
    "    base_channels: int=128,\n",
    "    channel_multipliers: tuple=(1, 2, 4, 8),\n",
    "    latent_dim: int=4,\n",
    "    num_res_blocks: int=2,\n",
    "    attn_resolutions: tuple=(16,),\n",
    "    dropout: float=0.0,\n",
    "    image_size: int=256\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self.conv_in = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "\n",
    "    channels = [base_channels * m for m in channel_multipliers]\n",
    "    current_res = image_size\n",
    "    in_ch = base_channels\n",
    "\n",
    "    self.down_blocks = nn.ModuleList()\n",
    "    for i, out_ch in enumerate(channels):\n",
    "      block = nn.ModuleList()\n",
    "      for _ in range(num_res_blocks):\n",
    "        block.append(ResBlock(in_ch, out_ch, dropout))\n",
    "        if current_res in attn_resolutions:\n",
    "          block.append(SelfAttention(out_ch))\n",
    "        in_ch = out_ch\n",
    "      \n",
    "      # Downsample except at last level\n",
    "      if i < len(channels) - 1:\n",
    "        block.append(DownSample(in_ch))\n",
    "        current_res //= 2\n",
    "      self.down_blocks.append(block)\n",
    "    \n",
    "    self.mid_block1 = ResBlock(in_ch, in_ch, dropout)\n",
    "    self.mid_attn = SelfAttention(in_ch)\n",
    "    self.mid_block2 = ResBlock(in_ch, in_ch, dropout)\n",
    "\n",
    "    self.norm_out = nn.GroupNorm(32, in_ch, eps=1e-6)\n",
    "    self.conv_out = nn.Conv2d(in_ch, 2 * latent_dim, 3, padding=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h = self.conv_in(x)\n",
    "    for block in self.down_blocks:\n",
    "      for layer in block:\n",
    "        h = layer(h)\n",
    "    h = self.mid_block2(self.mid_attn(self.mid_block1(h)))\n",
    "    h = self.conv_out(F.silu(self.norm_out(h)))\n",
    "    # Split into mean and log variance\n",
    "    mu, log_var = h.chunk(2, dim=1)\n",
    "    return mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Decoder ====================\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    out_channels: int=3,\n",
    "    base_channels: int=128,\n",
    "    channel_multipliers: tuple=(1, 2, 4, 8),\n",
    "    latent_dim: int=4,\n",
    "    num_res_blocks: int=2,\n",
    "    attn_resolutions: tuple=(16,),\n",
    "    dropout: float=0.0,\n",
    "    image_size: int=256\n",
    "  ):\n",
    "    super().__init__()\n",
    "    channels = [base_channels * m for m in reversed(channel_multipliers)]\n",
    "    in_ch = channels[0]\n",
    "    self.conv_in = nn.Conv2d(latent_dim, in_ch, 3, padding=1)\n",
    "\n",
    "    self.mid_block1 = ResBlock(in_ch, in_ch, dropout)\n",
    "    self.mid_attn   = SelfAttention(in_ch)\n",
    "    self.mid_block2 = ResBlock(in_ch, in_ch, dropout)\n",
    "\n",
    "    current_res = image_size // (2 ** (len(channel_multipliers) - 1))\n",
    "    self.up_blocks = nn.ModuleList()\n",
    "    for i, out_ch in enumerate(channels[1:] + [base_channels]):\n",
    "      block = nn.ModuleList()\n",
    "      for j in range(num_res_blocks + 1):   # +1 vs encoder is standard\n",
    "        block.append(ResBlock(in_ch, out_ch, dropout))\n",
    "        if current_res in attn_resolutions:\n",
    "          block.append(SelfAttention(out_ch))\n",
    "        in_ch = out_ch\n",
    "      if i < len(channels) - 1:\n",
    "        block.append(Upsample(in_ch))\n",
    "        current_res *= 2\n",
    "      self.up_blocks.append(block)\n",
    "\n",
    "    self.norm_out = nn.GroupNorm(32, in_ch, eps=1e-6)\n",
    "    self.conv_out = nn.Conv2d(in_ch, out_channels, 3, padding=1)\n",
    "  \n",
    "  def forward(self, z):\n",
    "    h = self.conv_in(z)\n",
    "    h = self.mid_block2(self.mid_attn(self.mid_block1(h)))\n",
    "    for block in self.up_blocks:\n",
    "      for layer in block:\n",
    "        h=layer(h)\n",
    "    return self.conv_out(F.silu(self.norm_out(h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility compute latent spatial size\n",
    "def compute_latent_size(image_size: int, channel_multipliers: tuple) -> int:\n",
    "  \"\"\"\n",
    "  Each stage (except the last) halves the spatial resolution.\n",
    "  e.g., image_size=256, 4 multipliers → 3 downsamples → latent is 32x32\n",
    "  \"\"\"\n",
    "  num_downsamples = len(channel_multipliers) - 1\n",
    "  latent_size = image_size // (2 ** num_downsamples)\n",
    "  assert image_size == latent_size * (2 ** num_downsamples), (\n",
    "    f\"image_size {image_size} is not evenly divisible by 2^{num_downsamples}. \"\n",
    "    f\"Use a power-of-2 image size.\"\n",
    "  )\n",
    "  return latent_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VAE ====================\n",
    "class VAE(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    in_channels: int = 3,\n",
    "    latent_dim: int = 4,\n",
    "    base_channels: int = 128,\n",
    "    channel_multipliers: tuple = (1, 2, 4, 8),\n",
    "    num_res_blocks: int = 2,\n",
    "    attn_resolutions: tuple = (16,),\n",
    "    dropout: float = 0.0,\n",
    "    image_size: int = 256,\n",
    "    # ── KEY TUNING PARAMETER ──\n",
    "    # β > 1  → stronger disentanglement, worse reconstruction\n",
    "    # β < 1  → better reconstruction, less structured latent space\n",
    "    # β = 1  → standard VAE\n",
    "    # For LDM-style (encoding for a downstream diffusion model), use β ≈ 1e-6\n",
    "    # to almost ignore KL and maximize reconstruction fidelity.\n",
    "    beta: float = 1.0,\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self.beta = beta\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "    enc_dec_kwargs = dict(\n",
    "      base_channels=base_channels,\n",
    "      channel_multipliers=channel_multipliers,\n",
    "      latent_dim=latent_dim,\n",
    "      num_res_blocks=num_res_blocks,\n",
    "      attn_resolutions=attn_resolutions,\n",
    "      dropout=dropout,\n",
    "      image_size=image_size,\n",
    "    )\n",
    "    self.encoder = Encoder(in_channels=in_channels, **enc_dec_kwargs)\n",
    "    self.decoder = Decoder(out_channels=in_channels, **enc_dec_kwargs)\n",
    "  \n",
    "  def reparameterize(self, mu, log_var):\n",
    "    \"\"\"\n",
    "    WATCH OUT: log_var should be clamped during training. Unclamped log_var\n",
    "    can explode to very large or very small values, causing NaN losses.\n",
    "    \"\"\"\n",
    "    log_var = log_var.clamp(-30.0, 20.0)\n",
    "    std = torch.exp(0.5 * log_var)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "  \n",
    "  def encode(self, x):\n",
    "    mu, log_var = self.encoder(x)\n",
    "    return mu, log_var  \n",
    "  \n",
    "  def decode(self, z):\n",
    "    return self.decoder(z)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    mu, log_var = self.encode(x)\n",
    "    z = self.reparameterize(mu, log_var)\n",
    "    x_hat = self.decode(z)\n",
    "    return x_hat, mu, log_var\n",
    "  \n",
    "  def compute_loss(self, x, x_hat, mu, log_var):\n",
    "    B = x.size(0)\n",
    "\n",
    "    # Per pixel MSE, averaged across all dimensions\n",
    "    recon_loss = F.mse_loss(x_hat, x, reduction='sum') / B\n",
    "\n",
    "    # KL Divergence (closed form) - standard normal\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / B\n",
    "\n",
    "    total_loss = recon_loss + self.beta * kl_loss\n",
    "    return total_loss, recon_loss, kl_loss\n",
    "  \n",
    "  @torch.no_grad()\n",
    "  def sample(self, num_samples: int, device: str='cuda'):\n",
    "    \"\"\"Sample from the prior p(z) = N(0, I)\"\"\"\n",
    "    # Need to know the latent spatial size at inference time\n",
    "    # 256 x 256 image gets downsampled to 16 x 16\n",
    "    latent_size = compute_latent_size(self.image_size, self.channel_multipliers)\n",
    "    z = torch.randn(num_samples, self.latent_dim, latent_size, latent_size, device=device)\n",
    "    return self.decode(z).clamp(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Key Training Parameters</u>\n",
    "\n",
    "- `lr`: 1e-4 is a safe default. Higher -> Faster, but unstable KL. Use warmup for first ~1000 steps.\n",
    "- `beta_warmup_steps`: Start $\\beta$ at 0 and linearly anneal to target. Without it model collapses the KL to 0 early and never learns a useful latent space (posterior collapse) \n",
    "- `gradient_clipping`: Clip to 1.0. VAEs can have explosive gradients from the KL term\n",
    "\n",
    "<u>WATCH OUT!</u>\n",
    "\n",
    "- KL loss should increase gradually. If it stays at 0 → posterior collapse.\n",
    "- Reconstruction loss should decrease steadily.\n",
    "- If recon loss plateaus very early → $\\beta$ is too high.\n",
    "- If generated samples have no structure → $\\beta$ is too low, latent space is not regularized.\n",
    "- Monitor $\\mu$ values: if ||$\\mu$|| >> 1, the encoder is ignoring the prior.\n",
    "- Monitor $\\sigma$ values: if $\\sigma$ → 0 everywhere, you have posterior collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAETrainer:\n",
    "  def __init__(\n",
    "    self,\n",
    "    model: VAE,\n",
    "    lr: float=1e-4,\n",
    "    beta_warmup_steps: int=10_000,\n",
    "    grad_clip: float=1.0,\n",
    "    device: str='cuda'\n",
    "  ):\n",
    "    self.model = model.to(device)\n",
    "    self.device = device\n",
    "    self.beta_warmup_steps = beta_warmup_steps\n",
    "    self.target_beta = model.beta\n",
    "    self.grad_clip = grad_clip\n",
    "    self.step = 0\n",
    "\n",
    "    self. optimizer = AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "  \n",
    "  def _current_beta(self):\n",
    "    \"\"\"Linear KL annealing — one of the most impactful tricks.\"\"\"\n",
    "    if self.beta_warmup_steps <= 0:\n",
    "      return self.target_beta\n",
    "    return self.target_beta * min(1.0, self.step / self.beta_warmup_steps)\n",
    "\n",
    "  def train_step(self, x: torch.Tensor):\n",
    "    self.model.train()\n",
    "    x = x.to(self.device)\n",
    "\n",
    "    self.model.beta = self._current_beta()\n",
    "\n",
    "    x_hat, mu, log_var = self.model(x)\n",
    "    total_loss, recon_loss, kl_loss = self.model.compute_loss(x, x_hat, mu, log_var)\n",
    "\n",
    "    self.optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "\n",
    "    # Gradient clipping — essential for stable VAE training\n",
    "    nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "\n",
    "    self.optimizer.step()\n",
    "    self.step += 1\n",
    "\n",
    "    return {\n",
    "      'total_loss': total_loss.item(),\n",
    "      'recon_loss': recon_loss.item(),\n",
    "      'kl_loss': kl_loss.item(),\n",
    "      'beta': self.model.beta,\n",
    "      'mu_mean': mu.mean().item(),\n",
    "      'mu_std': mu.std().item(),\n",
    "      'sigma_mean': (0.5 * log_var).exp().mean().item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model = VAE(\n",
    "#   in_channels=3,\n",
    "#   latent_dim=4,           # try 8 or 16 for more expressivity\n",
    "#   base_channels=64,       # 64 for fast experiments, 256 for SOTA\n",
    "#   channel_multipliers=(1, 2, 4, 8),  # 4 stages of 2x downsampling\n",
    "#   num_res_blocks=2,\n",
    "#   attn_resolutions=(16,),  # attention only at 16x16 feature maps\n",
    "#   dropout=0.0,             # dropout hurts VAE quality; only use if overfitting\n",
    "#   image_size=256,\n",
    "#   beta=1.0,                # tune this first\n",
    "# )\n",
    "\n",
    "# trainer = VAETrainer(\n",
    "#   model,\n",
    "#   lr=1e-4,\n",
    "#   beta_warmup_steps=10_000,   # increase for more complex datasets\n",
    "#   grad_clip=1.0,\n",
    "#   device=device,\n",
    "# )\n",
    "\n",
    "# # Simulate a training step\n",
    "# dummy_batch = torch.randn(4, 3, 256, 256)  # (B, C, H, W), normalized to [-1, 1]\n",
    "# metrics = trainer.train_step(dummy_batch)\n",
    "# print(metrics)\n",
    "\n",
    "# # Generation\n",
    "# samples = model.sample(4, device=device)\n",
    "# print(f\"Generated samples shape: {samples.shape}\")\n",
    "\n",
    "# # Count parameters\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"Total parameters: {total_params / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Data Loader ====================\n",
    "class ImageFolderDataset(Dataset):\n",
    "  \"\"\"\n",
    "  Loads all images from a folder (and optionally subfolders).\n",
    "  Resizes images to image_size x image_size and normalised to [-1, 1]\n",
    "  \"\"\"\n",
    "  def __init__(\n",
    "    self,\n",
    "    folder: str,\n",
    "    image_size: int = 256,\n",
    "    recursive: bool = True\n",
    "  ):\n",
    "    self.paths = []\n",
    "    folder = Path(folder)\n",
    "    glob = folder.rglob('*') if recursive else folder.glob('*')\n",
    "    for p in glob:\n",
    "      self.paths.append(p)\n",
    "    \n",
    "    if len(self.paths) == 0:\n",
    "      raise ValueError(f\"No images found in {folder}\")\n",
    "    \n",
    "    print(f\"Found {len(self.paths)} images in {folder}\")\n",
    "\n",
    "    self.transform = transforms.Compose([\n",
    "      transforms.Resize(image_size, interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "      transforms.CenterCrop(image_size),                      # Ensures square image\n",
    "      transforms.ToTensor(),                                  # [0, 255] → [0.0, 1.0]\n",
    "      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # → [-1.0, 1.0]\n",
    "    ])\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.paths)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    try:\n",
    "      img = Image.open(self.paths[idx]).convert('RGB')\n",
    "      return self.transform(img)\n",
    "    except Exception as e:\n",
    "      print(f\"Warning: failed to load {self.paths[idx]}: {e}. Returning zeros.\")\n",
    "      return torch.zeros(3, self.transform.transforms[1].size, self.transform.transforms[1].size)\n",
    "\n",
    "\n",
    "# ==================== Generator Helper ====================\n",
    "def save_samples(model, num_samples: int, save_path: str, device: str):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    samples = model.sample(num_samples, device=device)     # [-1, 1]\n",
    "    samples = (samples + 1) / 2                            # → [0, 1]\n",
    "    samples = samples.clamp(0, 1)\n",
    "  save_image(samples, save_path, nrow=int(math.sqrt(num_samples)))\n",
    "  print(f\"Saved {num_samples} samples → {save_path}\")\n",
    "\n",
    "def save_reconstructions(model, batch: torch.Tensor, save_path: str, device: str):\n",
    "  \"\"\"Save original vs reconstruction side-by-side for visual quality checks.\"\"\"\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    batch = batch.to(device)\n",
    "    x_hat, _, _ = model(batch)\n",
    "    comparison = torch.cat([batch, x_hat], dim=0)  # stack originals and recons\n",
    "    comparison = (comparison + 1) / 2\n",
    "    comparison = comparison.clamp(0, 1)\n",
    "  save_image(comparison, save_path, nrow=len(batch))\n",
    "  print(f\"Saved reconstructions → {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Training Loop\n",
    "# ----------------------------------------\n",
    "def train(\n",
    "  image_folder: str = \"./data/Celebrity Faces Dataset\",\n",
    "  output_dir: str = \"./data/outputs\",\n",
    "  # Image config\n",
    "  image_size: int = 256,\n",
    "\n",
    "  # Model config\n",
    "  latent_dim: int = 4,\n",
    "  base_channels: int = 64,\n",
    "  channel_multipliers: tuple = (1, 2, 4, 8),\n",
    "  num_res_blocks: int = 2,\n",
    "  attn_resolutions: tuple = (16,),\n",
    "  beta: float = 1.0,\n",
    "\n",
    "  # Training config\n",
    "  batch_size: int = 8,\n",
    "  num_epochs: int = 10,\n",
    "  lr: float = 1e-4,\n",
    "  beta_warmup_steps: int = 5000,\n",
    "  grad_clip: float = 1.0,\n",
    "\n",
    "  # Logging config\n",
    "  log_every: int = 50,          # print metrics every N steps\n",
    "  sample_every: int = 500,      # generate samples every N steps\n",
    "  save_every: int = 2000,       # save checkpoint every N steps\n",
    "  num_samples: int = 4,         # how many images to generate\n",
    "\n",
    "  device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "):\n",
    "  # ---- Setup --------------------------------\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  samples_dir = os.path.join(output_dir, 'samples')\n",
    "  recon_dir   = os.path.join(output_dir, 'reconstructions')\n",
    "  ckpt_dir    = os.path.join(output_dir, 'checkpoints')\n",
    "  for d in [samples_dir, recon_dir, ckpt_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "  latent_size = compute_latent_size(image_size, channel_multipliers)\n",
    "  print(f\"Image size: {image_size}x{image_size} → Latent size: {latent_size}x{latent_size}\")\n",
    "\n",
    "  # ---- Data ----------------------------------------\n",
    "  dataset = ImageFolderDataset(image_folder, image_size=image_size)\n",
    "  loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    "  )\n",
    "\n",
    "  # ---- Model ----------------------------------------\n",
    "  model = VAE(\n",
    "    in_channels=3,\n",
    "    latent_dim=latent_dim,\n",
    "    base_channels=base_channels,\n",
    "    channel_multipliers=channel_multipliers,\n",
    "    num_res_blocks=num_res_blocks,\n",
    "    attn_resolutions=attn_resolutions,\n",
    "    image_size=image_size,\n",
    "    beta=beta,\n",
    "  ).to(device)\n",
    "\n",
    "  model.image_size = image_size\n",
    "  model.channel_multipliers = channel_multipliers\n",
    "\n",
    "  total_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "  print(f\"Model parameters: {total_params:.1f}M\")\n",
    "\n",
    "  trainer = VAETrainer(\n",
    "    model,\n",
    "    lr=lr,\n",
    "    beta_warmup_steps=beta_warmup_steps,\n",
    "    grad_clip=grad_clip,\n",
    "    device=device,\n",
    "  )\n",
    "\n",
    "  val_batch = next(iter(loader))[:6] # Use the same 6 images as validation\n",
    "\n",
    "  global_step = 0\n",
    "  for epoch in range(num_epochs):\n",
    "    epoch_recon, epoch_kl, epoch_total = 0.0, 0.0, 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "      metrics = trainer.train_step(batch)\n",
    "      global_step += 1\n",
    "\n",
    "      epoch_recon += metrics['recon_loss']\n",
    "      epoch_kl    += metrics['kl_loss']\n",
    "      epoch_total += metrics['total_loss']\n",
    "\n",
    "      # ---- Logging ------------------------------\n",
    "      if global_step % log_every == 0:\n",
    "        print(\n",
    "          f\"Epoch {epoch+1:3d} | Step {global_step:6d} | \"\n",
    "          f\"Total {metrics['total_loss']:8.2f} | \"\n",
    "          f\"Recon {metrics['recon_loss']:8.2f} | \"\n",
    "          f\"KL {metrics['kl_loss']:7.3f} | \"\n",
    "          f\"β {metrics['beta']:.4f} | \"\n",
    "          f\"μ={metrics['mu_mean']:.3f} σ={metrics['sigma_mean']:.3f}\"\n",
    "        )\n",
    "      \n",
    "      # ---- Generate samples + reconstruction ------------------------------\n",
    "      if global_step % sample_every == 0:\n",
    "        save_samples(\n",
    "          model, num_samples,\n",
    "          save_path=os.path.join(samples_dir, f'step_{global_step:06d}.png'),\n",
    "          device=device,\n",
    "        )\n",
    "        save_reconstructions(\n",
    "          model, val_batch,\n",
    "          save_path=os.path.join(recon_dir, f'step_{global_step:06d}.png'),\n",
    "          device=device,\n",
    "        )\n",
    "    \n",
    "    n_batches = len(loader)\n",
    "    print(\n",
    "      f\"\\n── Epoch {epoch+1} Summary ──\\n\"\n",
    "      f\"   Avg Total: {epoch_total/n_batches:.2f} | \"\n",
    "      f\"   Avg Recon: {epoch_recon/n_batches:.2f} | \"\n",
    "      f\"   Avg KL:    {epoch_kl/n_batches:.3f}\\n\"\n",
    "    )\n",
    "\n",
    "  print(\"Training complete\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 256x256 → Latent size: 32x32\n",
      "Found 100 images in data/Celebrity Faces Dataset/Brad Pitt\n",
      "Model parameters: 39.6M\n",
      "\n",
      "── Epoch 1 Summary ──\n",
      "   Avg Total: 73261.36 |    Avg Recon: 72665.52 |    Avg KL:    31473.871\n",
      "\n",
      "\n",
      "── Epoch 2 Summary ──\n",
      "   Avg Total: 35096.31 |    Avg Recon: 34204.82 |    Avg KL:    5074.362\n",
      "\n",
      "\n",
      "── Epoch 3 Summary ──\n",
      "   Avg Total: 23495.90 |    Avg Recon: 22312.51 |    Avg KL:    4049.126\n",
      "\n",
      "\n",
      "── Epoch 4 Summary ──\n",
      "   Avg Total: 17062.72 |    Avg Recon: 15758.25 |    Avg KL:    3140.560\n",
      "\n",
      "Epoch   5 | Step     50 | Total 13889.32 | Recon 12415.05 | KL 3008.719 | β 0.4900 | μ=-0.043 σ=0.659\n",
      "\n",
      "── Epoch 5 Summary ──\n",
      "   Avg Total: 15359.81 |    Avg Recon: 13839.02 |    Avg KL:    2847.669\n",
      "\n",
      "\n",
      "── Epoch 6 Summary ──\n",
      "   Avg Total: 14337.84 |    Avg Recon: 12696.50 |    Avg KL:    2511.028\n",
      "\n",
      "\n",
      "── Epoch 7 Summary ──\n",
      "   Avg Total: 13512.02 |    Avg Recon: 11799.77 |    Avg KL:    2211.212\n",
      "\n",
      "\n",
      "── Epoch 8 Summary ──\n",
      "   Avg Total: 12980.11 |    Avg Recon: 11099.27 |    Avg KL:    2105.559\n",
      "\n",
      "Epoch   9 | Step    100 | Total 10467.50 | Recon  8612.45 | KL 1873.791 | β 0.9900 | μ=-0.009 σ=0.727\n",
      "\n",
      "── Epoch 9 Summary ──\n",
      "   Avg Total: 13031.49 |    Avg Recon: 11029.44 |    Avg KL:    2017.162\n",
      "\n",
      "\n",
      "── Epoch 10 Summary ──\n",
      "   Avg Total: 12294.54 |    Avg Recon: 10364.41 |    Avg KL:    1930.132\n",
      "\n",
      "\n",
      "── Epoch 11 Summary ──\n",
      "   Avg Total: 12733.52 |    Avg Recon: 10895.16 |    Avg KL:    1838.359\n",
      "\n",
      "\n",
      "── Epoch 12 Summary ──\n",
      "   Avg Total: 12690.72 |    Avg Recon: 10685.13 |    Avg KL:    2005.593\n",
      "\n",
      "Epoch  13 | Step    150 | Total  9048.79 | Recon  7192.03 | KL 1856.751 | β 1.0000 | μ=0.011 σ=0.780\n",
      "\n",
      "── Epoch 13 Summary ──\n",
      "   Avg Total: 11405.47 |    Avg Recon: 9593.70 |    Avg KL:    1811.776\n",
      "\n",
      "\n",
      "── Epoch 14 Summary ──\n",
      "   Avg Total: 10657.20 |    Avg Recon: 8850.28 |    Avg KL:    1806.919\n",
      "\n",
      "\n",
      "── Epoch 15 Summary ──\n",
      "   Avg Total: 11060.10 |    Avg Recon: 9230.11 |    Avg KL:    1829.986\n",
      "\n",
      "\n",
      "── Epoch 16 Summary ──\n",
      "   Avg Total: 10694.71 |    Avg Recon: 8895.17 |    Avg KL:    1799.548\n",
      "\n",
      "Epoch  17 | Step    200 | Total 11976.52 | Recon 10217.20 | KL 1759.317 | β 1.0000 | μ=-0.008 σ=0.772\n",
      "Saved 4 samples → ./data/outputs/samples/step_000200.png\n",
      "Saved reconstructions → ./data/outputs/reconstructions/step_000200.png\n",
      "\n",
      "── Epoch 17 Summary ──\n",
      "   Avg Total: 10541.05 |    Avg Recon: 8749.90 |    Avg KL:    1791.147\n",
      "\n",
      "\n",
      "── Epoch 18 Summary ──\n",
      "   Avg Total: 10284.69 |    Avg Recon: 8381.81 |    Avg KL:    1902.880\n",
      "\n",
      "\n",
      "── Epoch 19 Summary ──\n",
      "   Avg Total: 9677.54 |    Avg Recon: 7929.26 |    Avg KL:    1748.274\n",
      "\n",
      "\n",
      "── Epoch 20 Summary ──\n",
      "   Avg Total: 9574.15 |    Avg Recon: 7720.10 |    Avg KL:    1854.054\n",
      "\n",
      "Epoch  21 | Step    250 | Total  8711.66 | Recon  6606.02 | KL 2105.633 | β 1.0000 | μ=0.015 σ=0.751\n",
      "\n",
      "── Epoch 21 Summary ──\n",
      "   Avg Total: 9385.71 |    Avg Recon: 7583.84 |    Avg KL:    1801.866\n",
      "\n",
      "\n",
      "── Epoch 22 Summary ──\n",
      "   Avg Total: 10023.85 |    Avg Recon: 8275.86 |    Avg KL:    1747.995\n",
      "\n",
      "\n",
      "── Epoch 23 Summary ──\n",
      "   Avg Total: 9801.27 |    Avg Recon: 7859.98 |    Avg KL:    1941.292\n",
      "\n",
      "\n",
      "── Epoch 24 Summary ──\n",
      "   Avg Total: 9328.33 |    Avg Recon: 7417.53 |    Avg KL:    1910.804\n",
      "\n",
      "Epoch  25 | Step    300 | Total  8066.02 | Recon  6251.93 | KL 1814.088 | β 1.0000 | μ=0.001 σ=0.769\n",
      "\n",
      "── Epoch 25 Summary ──\n",
      "   Avg Total: 9099.08 |    Avg Recon: 7187.31 |    Avg KL:    1911.771\n",
      "\n",
      "\n",
      "── Epoch 26 Summary ──\n",
      "   Avg Total: 8683.86 |    Avg Recon: 6689.88 |    Avg KL:    1993.986\n",
      "\n",
      "\n",
      "── Epoch 27 Summary ──\n",
      "   Avg Total: 8396.15 |    Avg Recon: 6570.31 |    Avg KL:    1825.838\n",
      "\n",
      "\n",
      "── Epoch 28 Summary ──\n",
      "   Avg Total: 8596.07 |    Avg Recon: 6638.91 |    Avg KL:    1957.158\n",
      "\n",
      "\n",
      "── Epoch 29 Summary ──\n",
      "   Avg Total: 9220.05 |    Avg Recon: 7258.53 |    Avg KL:    1961.523\n",
      "\n",
      "Epoch  30 | Step    350 | Total  7157.34 | Recon  5199.61 | KL 1957.735 | β 1.0000 | μ=-0.068 σ=0.771\n",
      "\n",
      "── Epoch 30 Summary ──\n",
      "   Avg Total: 8421.22 |    Avg Recon: 6509.37 |    Avg KL:    1911.856\n",
      "\n",
      "\n",
      "── Epoch 31 Summary ──\n",
      "   Avg Total: 8402.24 |    Avg Recon: 6409.75 |    Avg KL:    1992.490\n",
      "\n",
      "\n",
      "── Epoch 32 Summary ──\n",
      "   Avg Total: 7603.56 |    Avg Recon: 5739.65 |    Avg KL:    1863.908\n",
      "\n",
      "\n",
      "── Epoch 33 Summary ──\n",
      "   Avg Total: 7454.37 |    Avg Recon: 5588.32 |    Avg KL:    1866.050\n",
      "\n",
      "Epoch  34 | Step    400 | Total  7105.97 | Recon  5407.08 | KL 1698.890 | β 1.0000 | μ=-0.052 σ=0.745\n",
      "Saved 4 samples → ./data/outputs/samples/step_000400.png\n",
      "Saved reconstructions → ./data/outputs/reconstructions/step_000400.png\n",
      "\n",
      "── Epoch 34 Summary ──\n",
      "   Avg Total: 7509.35 |    Avg Recon: 5610.14 |    Avg KL:    1899.209\n",
      "\n",
      "\n",
      "── Epoch 35 Summary ──\n",
      "   Avg Total: 7382.57 |    Avg Recon: 5460.18 |    Avg KL:    1922.393\n",
      "\n",
      "\n",
      "── Epoch 36 Summary ──\n",
      "   Avg Total: 7344.33 |    Avg Recon: 5469.18 |    Avg KL:    1875.147\n",
      "\n",
      "\n",
      "── Epoch 37 Summary ──\n",
      "   Avg Total: 7394.90 |    Avg Recon: 5480.86 |    Avg KL:    1914.045\n",
      "\n",
      "Epoch  38 | Step    450 | Total  7353.66 | Recon  5491.32 | KL 1862.337 | β 1.0000 | μ=-0.061 σ=0.760\n",
      "\n",
      "── Epoch 38 Summary ──\n",
      "   Avg Total: 7361.74 |    Avg Recon: 5419.86 |    Avg KL:    1941.883\n",
      "\n",
      "\n",
      "── Epoch 39 Summary ──\n",
      "   Avg Total: 7291.14 |    Avg Recon: 5373.27 |    Avg KL:    1917.868\n",
      "\n",
      "\n",
      "── Epoch 40 Summary ──\n",
      "   Avg Total: 7045.23 |    Avg Recon: 5100.62 |    Avg KL:    1944.605\n",
      "\n",
      "\n",
      "── Epoch 41 Summary ──\n",
      "   Avg Total: 6986.44 |    Avg Recon: 5122.50 |    Avg KL:    1863.942\n",
      "\n",
      "Epoch  42 | Step    500 | Total  6523.70 | Recon  4613.09 | KL 1910.607 | β 1.0000 | μ=0.010 σ=0.735\n",
      "\n",
      "── Epoch 42 Summary ──\n",
      "   Avg Total: 7368.45 |    Avg Recon: 5431.04 |    Avg KL:    1937.407\n",
      "\n",
      "\n",
      "── Epoch 43 Summary ──\n",
      "   Avg Total: 6925.50 |    Avg Recon: 4997.61 |    Avg KL:    1927.884\n",
      "\n",
      "\n",
      "── Epoch 44 Summary ──\n",
      "   Avg Total: 7139.64 |    Avg Recon: 5264.75 |    Avg KL:    1874.892\n",
      "\n",
      "\n",
      "── Epoch 45 Summary ──\n",
      "   Avg Total: 7499.10 |    Avg Recon: 5554.83 |    Avg KL:    1944.268\n",
      "\n",
      "Epoch  46 | Step    550 | Total  6679.09 | Recon  4802.21 | KL 1876.889 | β 1.0000 | μ=0.002 σ=0.750\n",
      "\n",
      "── Epoch 46 Summary ──\n",
      "   Avg Total: 6762.44 |    Avg Recon: 4862.94 |    Avg KL:    1899.497\n",
      "\n",
      "\n",
      "── Epoch 47 Summary ──\n",
      "   Avg Total: 7116.81 |    Avg Recon: 5241.26 |    Avg KL:    1875.556\n",
      "\n",
      "\n",
      "── Epoch 48 Summary ──\n",
      "   Avg Total: 6772.36 |    Avg Recon: 4797.11 |    Avg KL:    1975.245\n",
      "\n",
      "\n",
      "── Epoch 49 Summary ──\n",
      "   Avg Total: 6804.65 |    Avg Recon: 4933.82 |    Avg KL:    1870.835\n",
      "\n",
      "Epoch  50 | Step    600 | Total  6819.98 | Recon  4984.06 | KL 1835.922 | β 1.0000 | μ=0.033 σ=0.755\n",
      "Saved 4 samples → ./data/outputs/samples/step_000600.png\n",
      "Saved reconstructions → ./data/outputs/reconstructions/step_000600.png\n",
      "\n",
      "── Epoch 50 Summary ──\n",
      "   Avg Total: 7281.47 |    Avg Recon: 5304.75 |    Avg KL:    1976.719\n",
      "\n",
      "\n",
      "── Epoch 51 Summary ──\n",
      "   Avg Total: 6902.55 |    Avg Recon: 5004.63 |    Avg KL:    1897.922\n",
      "\n",
      "\n",
      "── Epoch 52 Summary ──\n",
      "   Avg Total: 7322.69 |    Avg Recon: 5374.47 |    Avg KL:    1948.224\n",
      "\n",
      "\n",
      "── Epoch 53 Summary ──\n",
      "   Avg Total: 7062.46 |    Avg Recon: 5091.32 |    Avg KL:    1971.144\n",
      "\n",
      "\n",
      "── Epoch 54 Summary ──\n",
      "   Avg Total: 6684.63 |    Avg Recon: 4813.27 |    Avg KL:    1871.356\n",
      "\n",
      "Epoch  55 | Step    650 | Total  5696.85 | Recon  3884.34 | KL 1812.514 | β 1.0000 | μ=-0.027 σ=0.742\n",
      "\n",
      "── Epoch 55 Summary ──\n",
      "   Avg Total: 6582.46 |    Avg Recon: 4632.76 |    Avg KL:    1949.702\n",
      "\n",
      "\n",
      "── Epoch 56 Summary ──\n",
      "   Avg Total: 6483.42 |    Avg Recon: 4544.74 |    Avg KL:    1938.684\n",
      "\n",
      "\n",
      "── Epoch 57 Summary ──\n",
      "   Avg Total: 6488.91 |    Avg Recon: 4569.31 |    Avg KL:    1919.595\n",
      "\n",
      "\n",
      "── Epoch 58 Summary ──\n",
      "   Avg Total: 6437.77 |    Avg Recon: 4529.43 |    Avg KL:    1908.339\n",
      "\n",
      "Epoch  59 | Step    700 | Total  6788.05 | Recon  4792.32 | KL 1995.729 | β 1.0000 | μ=0.035 σ=0.733\n",
      "\n",
      "── Epoch 59 Summary ──\n",
      "   Avg Total: 6649.56 |    Avg Recon: 4750.94 |    Avg KL:    1898.624\n",
      "\n",
      "\n",
      "── Epoch 60 Summary ──\n",
      "   Avg Total: 6261.66 |    Avg Recon: 4350.13 |    Avg KL:    1911.530\n",
      "\n",
      "\n",
      "── Epoch 61 Summary ──\n",
      "   Avg Total: 6348.84 |    Avg Recon: 4410.68 |    Avg KL:    1938.159\n",
      "\n",
      "\n",
      "── Epoch 62 Summary ──\n",
      "   Avg Total: 6152.56 |    Avg Recon: 4223.15 |    Avg KL:    1929.401\n",
      "\n",
      "Epoch  63 | Step    750 | Total  5841.86 | Recon  4072.32 | KL 1769.539 | β 1.0000 | μ=-0.020 σ=0.750\n",
      "\n",
      "── Epoch 63 Summary ──\n",
      "   Avg Total: 6139.28 |    Avg Recon: 4266.24 |    Avg KL:    1873.040\n",
      "\n",
      "\n",
      "── Epoch 64 Summary ──\n",
      "   Avg Total: 6450.48 |    Avg Recon: 4515.66 |    Avg KL:    1934.824\n",
      "\n",
      "\n",
      "── Epoch 65 Summary ──\n",
      "   Avg Total: 6477.80 |    Avg Recon: 4565.13 |    Avg KL:    1912.667\n",
      "\n",
      "\n",
      "── Epoch 66 Summary ──\n",
      "   Avg Total: 6741.35 |    Avg Recon: 4786.38 |    Avg KL:    1954.974\n",
      "\n",
      "Epoch  67 | Step    800 | Total  7143.68 | Recon  5125.59 | KL 2018.097 | β 1.0000 | μ=0.004 σ=0.751\n",
      "Saved 4 samples → ./data/outputs/samples/step_000800.png\n",
      "Saved reconstructions → ./data/outputs/reconstructions/step_000800.png\n",
      "\n",
      "── Epoch 67 Summary ──\n",
      "   Avg Total: 6664.26 |    Avg Recon: 4730.58 |    Avg KL:    1933.686\n",
      "\n",
      "\n",
      "── Epoch 68 Summary ──\n",
      "   Avg Total: 6503.19 |    Avg Recon: 4533.45 |    Avg KL:    1969.743\n",
      "\n",
      "\n",
      "── Epoch 69 Summary ──\n",
      "   Avg Total: 6301.00 |    Avg Recon: 4386.74 |    Avg KL:    1914.263\n",
      "\n",
      "\n",
      "── Epoch 70 Summary ──\n",
      "   Avg Total: 6558.14 |    Avg Recon: 4624.96 |    Avg KL:    1933.177\n",
      "\n",
      "Epoch  71 | Step    850 | Total  6787.10 | Recon  4906.69 | KL 1880.407 | β 1.0000 | μ=-0.002 σ=0.738\n",
      "\n",
      "── Epoch 71 Summary ──\n",
      "   Avg Total: 6456.54 |    Avg Recon: 4544.93 |    Avg KL:    1911.607\n",
      "\n",
      "\n",
      "── Epoch 72 Summary ──\n",
      "   Avg Total: 6280.81 |    Avg Recon: 4273.20 |    Avg KL:    2007.609\n",
      "\n",
      "\n",
      "── Epoch 73 Summary ──\n",
      "   Avg Total: 6177.65 |    Avg Recon: 4294.05 |    Avg KL:    1883.600\n",
      "\n",
      "\n",
      "── Epoch 74 Summary ──\n",
      "   Avg Total: 6408.40 |    Avg Recon: 4441.71 |    Avg KL:    1966.695\n",
      "\n",
      "Epoch  75 | Step    900 | Total  6096.37 | Recon  4128.36 | KL 1968.007 | β 1.0000 | μ=-0.031 σ=0.729\n",
      "\n",
      "── Epoch 75 Summary ──\n",
      "   Avg Total: 6124.06 |    Avg Recon: 4175.39 |    Avg KL:    1948.670\n",
      "\n",
      "\n",
      "── Epoch 76 Summary ──\n",
      "   Avg Total: 5891.63 |    Avg Recon: 3998.05 |    Avg KL:    1893.572\n",
      "\n",
      "\n",
      "── Epoch 77 Summary ──\n",
      "   Avg Total: 6014.87 |    Avg Recon: 4118.82 |    Avg KL:    1896.052\n",
      "\n",
      "\n",
      "── Epoch 78 Summary ──\n",
      "   Avg Total: 6109.96 |    Avg Recon: 4221.69 |    Avg KL:    1888.271\n",
      "\n",
      "\n",
      "── Epoch 79 Summary ──\n",
      "   Avg Total: 6351.44 |    Avg Recon: 4401.72 |    Avg KL:    1949.723\n",
      "\n",
      "Epoch  80 | Step    950 | Total  5461.88 | Recon  3667.25 | KL 1794.630 | β 1.0000 | μ=-0.021 σ=0.732\n",
      "\n",
      "── Epoch 80 Summary ──\n",
      "   Avg Total: 6176.65 |    Avg Recon: 4253.67 |    Avg KL:    1922.979\n",
      "\n",
      "\n",
      "── Epoch 81 Summary ──\n",
      "   Avg Total: 5958.38 |    Avg Recon: 4033.04 |    Avg KL:    1925.336\n",
      "\n",
      "\n",
      "── Epoch 82 Summary ──\n",
      "   Avg Total: 5848.56 |    Avg Recon: 3966.63 |    Avg KL:    1881.927\n",
      "\n",
      "\n",
      "── Epoch 83 Summary ──\n",
      "   Avg Total: 5848.94 |    Avg Recon: 3952.34 |    Avg KL:    1896.603\n",
      "\n",
      "Epoch  84 | Step   1000 | Total  6520.50 | Recon  4524.61 | KL 1995.889 | β 1.0000 | μ=0.054 σ=0.731\n",
      "Saved 4 samples → ./data/outputs/samples/step_001000.png\n",
      "Saved reconstructions → ./data/outputs/reconstructions/step_001000.png\n",
      "\n",
      "── Epoch 84 Summary ──\n",
      "   Avg Total: 6057.64 |    Avg Recon: 4140.93 |    Avg KL:    1916.711\n",
      "\n",
      "\n",
      "── Epoch 85 Summary ──\n",
      "   Avg Total: 5745.33 |    Avg Recon: 3877.09 |    Avg KL:    1868.237\n",
      "\n",
      "\n",
      "── Epoch 86 Summary ──\n",
      "   Avg Total: 6142.43 |    Avg Recon: 4226.54 |    Avg KL:    1915.890\n",
      "\n",
      "\n",
      "── Epoch 87 Summary ──\n",
      "   Avg Total: 6140.65 |    Avg Recon: 4197.81 |    Avg KL:    1942.849\n",
      "\n",
      "Epoch  88 | Step   1050 | Total  5485.21 | Recon  3445.98 | KL 2039.227 | β 1.0000 | μ=0.019 σ=0.727\n",
      "\n",
      "── Epoch 88 Summary ──\n",
      "   Avg Total: 5708.71 |    Avg Recon: 3779.87 |    Avg KL:    1928.838\n",
      "\n",
      "\n",
      "── Epoch 89 Summary ──\n",
      "   Avg Total: 5575.13 |    Avg Recon: 3666.06 |    Avg KL:    1909.067\n",
      "\n",
      "\n",
      "── Epoch 90 Summary ──\n",
      "   Avg Total: 5839.69 |    Avg Recon: 3928.83 |    Avg KL:    1910.865\n",
      "\n",
      "\n",
      "── Epoch 91 Summary ──\n",
      "   Avg Total: 5565.53 |    Avg Recon: 3658.95 |    Avg KL:    1906.578\n",
      "\n",
      "Epoch  92 | Step   1100 | Total  5786.13 | Recon  3825.76 | KL 1960.375 | β 1.0000 | μ=-0.016 σ=0.727\n",
      "\n",
      "── Epoch 92 Summary ──\n",
      "   Avg Total: 5645.84 |    Avg Recon: 3735.19 |    Avg KL:    1910.645\n",
      "\n",
      "\n",
      "── Epoch 93 Summary ──\n",
      "   Avg Total: 5659.23 |    Avg Recon: 3764.90 |    Avg KL:    1894.328\n",
      "\n",
      "\n",
      "── Epoch 94 Summary ──\n",
      "   Avg Total: 6166.54 |    Avg Recon: 4247.22 |    Avg KL:    1919.319\n",
      "\n",
      "\n",
      "── Epoch 95 Summary ──\n",
      "   Avg Total: 6173.71 |    Avg Recon: 4278.43 |    Avg KL:    1895.289\n",
      "\n",
      "Epoch  96 | Step   1150 | Total  5507.79 | Recon  3712.71 | KL 1795.073 | β 1.0000 | μ=-0.022 σ=0.738\n",
      "\n",
      "── Epoch 96 Summary ──\n",
      "   Avg Total: 5756.27 |    Avg Recon: 3815.42 |    Avg KL:    1940.848\n",
      "\n",
      "\n",
      "── Epoch 97 Summary ──\n",
      "   Avg Total: 5775.98 |    Avg Recon: 3869.98 |    Avg KL:    1906.002\n",
      "\n",
      "\n",
      "── Epoch 98 Summary ──\n",
      "   Avg Total: 5801.63 |    Avg Recon: 3876.51 |    Avg KL:    1925.119\n",
      "\n",
      "\n",
      "── Epoch 99 Summary ──\n",
      "   Avg Total: 5613.76 |    Avg Recon: 3740.56 |    Avg KL:    1873.198\n",
      "\n",
      "Epoch 100 | Step   1200 | Total  5549.49 | Recon  3652.89 | KL 1896.604 | β 1.0000 | μ=0.012 σ=0.740\n",
      "Saved 4 samples → ./data/outputs/samples/step_001200.png\n",
      "Saved reconstructions → ./data/outputs/reconstructions/step_001200.png\n",
      "\n",
      "── Epoch 100 Summary ──\n",
      "   Avg Total: 5555.60 |    Avg Recon: 3662.98 |    Avg KL:    1892.626\n",
      "\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "  image_folder=\"./data/Celebrity Faces Dataset/Brad Pitt\",\n",
    "  output_dir=\"./data/outputs\",\n",
    "  image_size=256,\n",
    "  batch_size=8,\n",
    "  num_epochs=100,\n",
    "  beta=1.0,\n",
    "  beta_warmup_steps=100,\n",
    "  sample_every=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GANs)\n",
    "\n",
    "Dataset: [ Pokemon Images ](https://www.kaggle.com/datasets/kvpratama/pokemon-images-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
