{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "\n",
    "Learn about the `tokenizers` library from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 17/12/2025   | Martin | Create  | Notebook created for Ch6 Tokenizers | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Training a Tokenizer](#training-a-tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Learn how to train a brand new tokenizer on a corpus of texts, so it can then be used to pretrain a language model\n",
    "\n",
    "<u>Questions to Answer</u>\n",
    "\n",
    "- How to train a new tokenizer similar to the one used by a given checkpoint on a new corpus of texts\n",
    "- The special features of fast tokenizers\n",
    "- The differences between the three main subword tokenization algorithms used in NLP today\n",
    "- How to build a tokenizer from scratch with the ðŸ¤— Tokenizers library and train it on some data\n",
    "\n",
    "> Training a tokenizer is a statistical process that tries to identify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on the tokenization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Tokenizer\n",
    "\n",
    "1. Assemble a corpus of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"code_search_net\", \"default\", revision=\"refs/convert/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the whole function string\n",
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load batches of text - Create Python generator\n",
    "def get_training_corpus():\n",
    "  training_corpus = (\n",
    "    raw_datasets['train'][i, i+1000]['whole_func_string']\n",
    "    for i in range(len(raw_datasets['train']), 1000)\n",
    "  )\n",
    "  return training_corpus\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative version to create corpus\n",
    "def get_training_corpus():\n",
    "  dataset = raw_datasets[\"train\"]\n",
    "  for start_idx in range(0, len(dataset), 1000):\n",
    "    samples = dataset[start_idx : start_idx + 1000]\n",
    "    yield samples[\"whole_func_string\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a new tokenizer:\n",
    "\n",
    "- Don't retrain from scratch. Need to learn unique tokens for downstream use.\n",
    "- `train_new_from_iterator` - Function used to train a new corpus or iterator datatype. Must be a \"fast\" tokenizer\n",
    "- `AutoTokenizer` will automatically select the \"fast\" tokenizer if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "  \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "  return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorter tokens on new tokenizer compared to old one\n",
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")\n",
    "\n",
    "# Can also push to HF repo\n",
    "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Tokenizers\n",
    "\n",
    "- __Slow Tokenizers__ - Written in Python\n",
    "- __Fast Tokenizers__ - Written in Rust (much fast)\n",
    "\n",
    "> Only when tokenizing lots of texts in parallel at the same time that you will be able to clearly see the difference\n",
    "\n",
    "- _Offset Mapping:_ Keeps track of the original span of texts the final tokens come from\n",
    "  - Allows accessing more granular details about the subword positions and relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "print(type(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.is_fast) # Check if using fast or slow\n",
    "print(encoding.tokens()) # Access tokens without converting back\n",
    "print(encoding.word_ids()) # Indicates which subwords belong to the same word\n",
    "start, end = encoding.word_to_chars(3) # Convert the tokens back to words\n",
    "example[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token classification pipeline\n",
    "\n",
    "Understand the post-processing portion once tokens are selected, in _Named Entity Recognition (NER)_ task.\n",
    "\n",
    "Uses: `dbmdz/bert-large-cased-finetuned-conll03-english`\n",
    "\n",
    "- `aggregation_strategy`:\n",
    "  - `\"simple\"`: Mean\n",
    "  - `\"max\"`: Max of tokens\n",
    "  - `\"average\"`: Average of scores from individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can remove \"aggregation_strategy\" for subword tokens\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making input predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint)\n",
    "\n",
    "example = \"My name is Martin and I am a student at the University of Michigan, but staying in Singapore\"\n",
    "inputs = tokenizer(example, return_tensors='pt')\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 class labels\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to class labels\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "preds = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "print(preds)\n",
    "\n",
    "results = []\n",
    "tokens = inputs.tokens()\n",
    "for idx, pred in enumerate(preds):\n",
    "  label = model.config.id2label[pred]\n",
    "  if label != \"O\":\n",
    "    results.append(\n",
    "      {'entity': label, 'score': probs[idx][pred], 'word': token[idx]}\n",
    "    )\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine individual tokens into full words and calculate the score\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "  pred = predictions[idx]\n",
    "  label = model.config.id2label[pred]\n",
    "  if label != \"O\":\n",
    "    # Remove the B- or I-\n",
    "    label = label[2:]\n",
    "    start, _ = offsets[idx]\n",
    "\n",
    "    # Grab all the tokens labeled with I-label\n",
    "    all_scores = []\n",
    "    while (\n",
    "      idx < len(predictions)\n",
    "      and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "    ):\n",
    "      all_scores.append(probabilities[idx][pred])\n",
    "      _, end = offsets[idx]\n",
    "      idx += 1\n",
    "\n",
    "    # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "    score = np.mean(all_scores).item()\n",
    "    word = example[start:end]\n",
    "    results.append(\n",
    "      {\n",
    "        \"entity_group\": label,\n",
    "        \"score\": score,\n",
    "        \"word\": word,\n",
    "        \"start\": start,\n",
    "        \"end\": end,\n",
    "      }\n",
    "    )\n",
    "  idx += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-Answering pipeline\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2025-06-18T19:03:45.452311+08:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.9\n",
      "IPython version      : 8.31.0\n",
      "\n",
      "Compiler    : MSC v.1938 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel\n",
      "CPU cores   : 20\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_venv",
   "language": "python",
   "name": "py311_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
