{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using Transformers\n",
    "\n",
    "Learning the basics of `Transformers` library from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 18/11/2025   | Martin | Created   | Notebook created to explore Transformers library | \n",
    "| 19/11/2025   | Martin | Update   |  Completed basic usage of transformer library. Working on Inferenced Deploymnent |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Models](#models)\n",
    "* [Tokenizers](#tokenizers)\n",
    "* [Putting it Together](#putting-it-together)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "  pipeline,\n",
    "  infer_device,\n",
    "  AutoTokenizer,\n",
    "  AutoModel,\n",
    "  AutoModelForSequenceClassification\n",
    ")\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9754351f2d04dab85c829f29f7b054f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Software\\venv\\py311_venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6eda0c3f5c94f52a6fd2bb7648b32e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07bcca97b3945c38eb28b082f1bf92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c0efdaaf924c92b8c7cbdc61936298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9995144605636597},\n",
       " {'label': 'POSITIVE', 'score': 0.9972779154777527}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = infer_device()\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\n",
    "  \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "  \"I hate this so much\",\n",
    "  \"Dam my glorious king stephen wardell curry\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543eac4f8b10454888db498326d663c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce0ae1e65434c4a90bdf74d4f4b70ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94bbe86b17742f8a1376263c367b4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  5477,  2026, 14013,  2332,  4459,  4829,  5349, 15478,   102,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "  \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "  \"I hate this so much\",\n",
    "  \"Dam my glorious king stephen wardell curry\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='pt')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `input_ids`: Unique identifiers of the tokens in each sentence\n",
    "- `attention_mask`: Hides token weights that should not be available for the prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the pretrained model\n",
    "\n",
    "- Contains base transformer model: `hidden_states` and `features`\n",
    "- Retrieve high-dimensional vector representation - contextual understanding of that input by the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
      "embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "embeddings.LayerNorm.weight torch.Size([768])\n",
      "embeddings.LayerNorm.bias torch.Size([768])\n",
      "transformer.layer.0.attention.q_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.0.attention.q_lin.bias torch.Size([768])\n",
      "transformer.layer.0.attention.k_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.0.attention.k_lin.bias torch.Size([768])\n",
      "transformer.layer.0.attention.v_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.0.attention.v_lin.bias torch.Size([768])\n",
      "transformer.layer.0.attention.out_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.0.attention.out_lin.bias torch.Size([768])\n",
      "transformer.layer.0.sa_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.0.sa_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.0.ffn.lin1.weight torch.Size([3072, 768])\n",
      "transformer.layer.0.ffn.lin1.bias torch.Size([3072])\n",
      "transformer.layer.0.ffn.lin2.weight torch.Size([768, 3072])\n",
      "transformer.layer.0.ffn.lin2.bias torch.Size([768])\n",
      "transformer.layer.0.output_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.0.output_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.1.attention.q_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.1.attention.q_lin.bias torch.Size([768])\n",
      "transformer.layer.1.attention.k_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.1.attention.k_lin.bias torch.Size([768])\n",
      "transformer.layer.1.attention.v_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.1.attention.v_lin.bias torch.Size([768])\n",
      "transformer.layer.1.attention.out_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.1.attention.out_lin.bias torch.Size([768])\n",
      "transformer.layer.1.sa_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.1.sa_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.1.ffn.lin1.weight torch.Size([3072, 768])\n",
      "transformer.layer.1.ffn.lin1.bias torch.Size([3072])\n",
      "transformer.layer.1.ffn.lin2.weight torch.Size([768, 3072])\n",
      "transformer.layer.1.ffn.lin2.bias torch.Size([768])\n",
      "transformer.layer.1.output_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.1.output_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.2.attention.q_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.2.attention.q_lin.bias torch.Size([768])\n",
      "transformer.layer.2.attention.k_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.2.attention.k_lin.bias torch.Size([768])\n",
      "transformer.layer.2.attention.v_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.2.attention.v_lin.bias torch.Size([768])\n",
      "transformer.layer.2.attention.out_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.2.attention.out_lin.bias torch.Size([768])\n",
      "transformer.layer.2.sa_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.2.sa_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.2.ffn.lin1.weight torch.Size([3072, 768])\n",
      "transformer.layer.2.ffn.lin1.bias torch.Size([3072])\n",
      "transformer.layer.2.ffn.lin2.weight torch.Size([768, 3072])\n",
      "transformer.layer.2.ffn.lin2.bias torch.Size([768])\n",
      "transformer.layer.2.output_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.2.output_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.3.attention.q_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.3.attention.q_lin.bias torch.Size([768])\n",
      "transformer.layer.3.attention.k_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.3.attention.k_lin.bias torch.Size([768])\n",
      "transformer.layer.3.attention.v_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.3.attention.v_lin.bias torch.Size([768])\n",
      "transformer.layer.3.attention.out_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.3.attention.out_lin.bias torch.Size([768])\n",
      "transformer.layer.3.sa_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.3.sa_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.3.ffn.lin1.weight torch.Size([3072, 768])\n",
      "transformer.layer.3.ffn.lin1.bias torch.Size([3072])\n",
      "transformer.layer.3.ffn.lin2.weight torch.Size([768, 3072])\n",
      "transformer.layer.3.ffn.lin2.bias torch.Size([768])\n",
      "transformer.layer.3.output_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.3.output_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.4.attention.q_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.4.attention.q_lin.bias torch.Size([768])\n",
      "transformer.layer.4.attention.k_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.4.attention.k_lin.bias torch.Size([768])\n",
      "transformer.layer.4.attention.v_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.4.attention.v_lin.bias torch.Size([768])\n",
      "transformer.layer.4.attention.out_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.4.attention.out_lin.bias torch.Size([768])\n",
      "transformer.layer.4.sa_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.4.sa_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.4.ffn.lin1.weight torch.Size([3072, 768])\n",
      "transformer.layer.4.ffn.lin1.bias torch.Size([3072])\n",
      "transformer.layer.4.ffn.lin2.weight torch.Size([768, 3072])\n",
      "transformer.layer.4.ffn.lin2.bias torch.Size([768])\n",
      "transformer.layer.4.output_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.4.output_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.5.attention.q_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.5.attention.q_lin.bias torch.Size([768])\n",
      "transformer.layer.5.attention.k_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.5.attention.k_lin.bias torch.Size([768])\n",
      "transformer.layer.5.attention.v_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.5.attention.v_lin.bias torch.Size([768])\n",
      "transformer.layer.5.attention.out_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.5.attention.out_lin.bias torch.Size([768])\n",
      "transformer.layer.5.sa_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.5.sa_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.5.ffn.lin1.weight torch.Size([3072, 768])\n",
      "transformer.layer.5.ffn.lin1.bias torch.Size([3072])\n",
      "transformer.layer.5.ffn.lin2.weight torch.Size([768, 3072])\n",
      "transformer.layer.5.ffn.lin2.bias torch.Size([768])\n",
      "transformer.layer.5.output_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.5.output_layer_norm.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "sd = model.state_dict()\n",
    "for k, v in sd.items():\n",
    "  print(k, v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0195e-02, 9.5980e-01],\n",
       "        [9.9951e-01, 4.8549e-04],\n",
       "        [2.7221e-03, 9.9728e-01]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = F.softmax(outputs.logits, dim=-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "`AutoModel` class provides a simplied way to instantiate any model from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4865edb85ec0421c82bebd4225c3cc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b46253f311e49d4ae1f31a60ab925ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loadin a model using `save_pretrained` and `from_pretrained`\n",
    "\n",
    "- Saves to a directory containing 2 files:\n",
    "\n",
    "1. `config.json` containing attributes needed to build the model architecture and metadata\n",
    "2. `model.safetensors` is the state dict containing the model weights\n",
    "\n",
    "Both files should be stored in the same folder to load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = AutoModel.from_pretrained(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pushing a model to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bb120d4f5e41828475ffc8b18c5841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d48d4daeff4b4f9e5d4b39e7615e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Minimartzz/my-awesome-model/commit/9b9901d2fbb9a736f7d9f757ba9cd7e3b1eb1c4f', commit_message='Upload model', commit_description='', oid='9b9901d2fbb9a736f7d9f757ba9cd7e3b1eb1c4f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Minimartzz/my-awesome-model', endpoint='https://huggingface.co', repo_type='model', repo_id='Minimartzz/my-awesome-model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('my-awesome-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0571f759d0b4b5db0186810e15ca4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbd28d9d9a14674a48d70c6a6f6b010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the model from the hub\n",
    "model = AutoModel.from_pretrained(\"Minimartzz/my-awesome-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "\n",
    "Returns a dictionary:\n",
    "\n",
    "- `input_ids`: numerical representation of tokens\n",
    "- `token_type_ids`: tell the model which part of the input is sentence A and which is sentence B\n",
    "- `attention_mask`: indicates which tokens should be attended to and which should not\n",
    "\n",
    "When decoding there are additional special tokens appended to indicate the type of task required to be performed by the model. This is unqiue to the specific model used.\n",
    "\n",
    "`\"[CLS] Hello, I'm a single sentence! [SEP]\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff91b97e89e44c9d9884282cce83ecd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878cc8344b5841c58c935f94c662575c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f83e441d3a14efdb36204a62a1b7ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 117, 146, 1821, 170, 3163, 1591, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] Hello, I am a basketball player [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "encoded_input = tokenizer(\"Hello, I am a basketball player\")\n",
    "print(encoded_input)\n",
    "\n",
    "decoded_input = tokenizer.decode(encoded_input[\"input_ids\"])\n",
    "print(decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 8667,  117,  146,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With additional configuration\n",
    "encoded_add = tokenizer(\n",
    "  \"Hello, I am a basketball player\",\n",
    "  padding=True,\n",
    "  truncation=True,\n",
    "  max_length=5,\n",
    "  return_tensors='pt'\n",
    ")\n",
    "encoded_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'cow', ',', 'hear', 'me', 'm', '##oo']\n",
      "[146, 1821, 170, 13991, 117, 2100, 1143, 182, 5658]\n"
     ]
    }
   ],
   "source": [
    "# 1. Tokenisation\n",
    "# 2. Input IDs\n",
    "sequence = \"I am a cow, hear me moo\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a cow, hear me moo'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple sequences\n",
    "\n",
    "Transformer models expect multiple sentence by default i.e all data sent must have at least batch size of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:  tensor([[ 3.5694, -2.8733]], grad_fn=<AddmmBackward0>)\n",
      "Probs: tensor([[0.9984, 0.0016]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "x = torch.tensor([ids])\n",
    "out = model(x)\n",
    "\n",
    "print(\"Logits:\", out.logits)\n",
    "print(\"Probs:\", F.softmax(out.logits, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For padding use tokenizer.pad_token_id\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Attention mask\n",
    "batched_ids = [\n",
    "  [200, 200, 200],\n",
    "  [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "  [1, 1, 1],\n",
    "  [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"What do you want from me?!\", \"This food is p mehh\", \"That was a sick game!\"]\n",
    "\n",
    "tokens = tokenizer(\n",
    "  sequences,\n",
    "  padding=True,\n",
    "  truncation=True,\n",
    "  return_tensors='pt'\n",
    ")\n",
    "\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.7726e-01, 2.2741e-02],\n",
       "        [1.1789e-02, 9.8821e-01],\n",
       "        [9.9968e-01, 3.2245e-04]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(output.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployments\n",
    "\n",
    "Testing various to call models from different methods of deployments\n",
    "\n",
    "1. TGI\n",
    "2. vLLM\n",
    "3. llama.cpp\n",
    "\n",
    "Refer here for more details on sample deployments: https://huggingface.co/learn/llm-course/en/chapter2/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TGI\n",
    "\n",
    "TGI model hosted in docker container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docker run --platform linux/amd64 \\\n",
    "  --shm-size 1g \\\n",
    "  -p 8080:80 \\\n",
    "  -v ~/.cache/huggingface:/data \\\n",
    "  ghcr.io/huggingface/text-generation-inference:latest \\\n",
    "  --model-id HuggingFaceTB/SmolLM2-360M-Instruct\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalise client to point at TGI endpoint\n",
    "client = InferenceClient(\n",
    "  model=\"http://localhost:8080\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation task\n",
    "response = client.text_generation(\n",
    "  \"Tell me a story\",\n",
    "  max_new_tokens=100,\n",
    "  temperature=0.7,\n",
    "  top_p=0.9,\n",
    "  details=True,\n",
    "  stop_sequences=[]\n",
    ")\n",
    "print(response.generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat task\n",
    "response = client.chat_completion(\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a story\"}\n",
    "  ],\n",
    "  max_tokens=100,\n",
    "  temperature=0.7,\n",
    "  top_p=0.9\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. llama.cpp\n",
    "\n",
    "Deployment requires installation and build of the llama.cpp interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client pointing to llama.cpp server\n",
    "client = InferenceClient(\n",
    "  model=\"http://localhost:8080/v1\",  # URL to the llama.cpp server\n",
    "  token=\"sk-no-key-required\",  # llama.cpp server requires this placeholder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_venv",
   "language": "python",
   "name": "py311_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
