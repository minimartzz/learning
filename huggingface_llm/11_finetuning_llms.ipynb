{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning LLMs\n",
    "\n",
    "Supervised fine-tuning (SFT) makes models more versatile by adjusting their responses to more accurately match a generalised task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 19/01/2026   | Martin | Created   | Notebook created to explore finetuning LLMs. Done chat templates and supervised fine-tuning | \n",
    "| 21/01/2026   | Martin | Update   | Completed chapter. Finished LoRA and Evaluation sections | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [1. Chat Templates](#1-chat-templates)\n",
    "* [2. Supervised Fine-tuning](#2-supervised-fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "LLMs undergo SFT to make them more helpful and aligned with human preferences. Generally 4 steps are used:\n",
    "\n",
    "1. __Chat Templates__ - Data used: Structured interactiosn between users and AI models, to ensuring consistent and contextually appropriate responses\n",
    "2. __Supervised Fine-tuning__ - Training the model on the task-specific dataset with labeled examples\n",
    "3. __LoRA__ - Technique to improve fine-tuned model performance. It adds low-rank matrices to the model's layers substituting large matrix transformations while preseving the models' pre-trained knowledge. Offers good memory saving capabilities.\n",
    "4. __Evaluation__ - Measure the performance of the model on a task-specific dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Chat Templates\n",
    "\n",
    "Format conversations to direct the model how to respond. They are crucial for:\n",
    "\n",
    "- Maintaining consistent conversation structure\n",
    "- Ensuring proper role identification\n",
    "- Managing context across multiple turns\n",
    "- Supporting advanced features like tool use\n",
    "\n",
    "<u>Base vs. Instruct</u>\n",
    "\n",
    "- _Base Models:_ Are result of training on the large corpus of text. They only perform causal prediction by guessing the next most likely word\n",
    "- _Instruct Models_: Trained to follow specific conversational structure. Can handle more complex interactions (e.g tool use, multimodal input, and function calling)\n",
    "\n",
    "## ChatML template\n",
    "\n",
    "Format for conversation with clear role indicators. [ChatML template](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/blob/e2c3f7557efbdec707ae3a336371d169783f1da1/tokenizer_config.json#L146)\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"Hi! How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"What's the weather?\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Is converted to\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "Hello!<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Hi! How can I help you today?<|im_end|>\n",
    "<|im_start|>user\n",
    "What's the weather?<|im_start|>assistant\n",
    "```\n",
    "\n",
    "ðŸš¨ALERT: Each model has it's own template used to structure conversations. Always look at their specs before implementing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load different tokenizers to observe their different templates\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "smol_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "]\n",
    "\n",
    "# Each will format according to its model's template\n",
    "mistral_chat = mistral_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "smol_chat = smol_tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> [INST] You are a helpful assistant.\\n\\nHello! [/INST]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHello!<|im_end|>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smol_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional features\n",
    "\n",
    "1. __Tool Use__ -  When models need to interact with external tools or APIs\n",
    "2. __Multimodal Inputs__ - For handling images, audio, or other media types\n",
    "3. __Function Calling__ - For structured function execution\n",
    "4. __Multi-turn Context__ - For maintaining conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multimodal conversation (using images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful vision assistant that can analyze images.\",\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "      {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "      # Image URL passed to be included in prompt\n",
    "      {\"type\": \"image\", \"image_url\": \"https://hips.hearstapps.com/hmg-prod/images/dog-puppy-on-garden-royalty-free-image-1586966191.jpg?crop=1.00xw:0.669xh;0,0.190xh&resize=1200:*\"},\n",
    "    ],\n",
    "  },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are an AI assistant that can use tools. Available tools: calculator, weather_api\",\n",
    "  },\n",
    "  {\"role\": \"user\", \"content\": \"What's 123 * 456 and is it raining in Paris?\"},\n",
    "  {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Let me help you with that.\",\n",
    "    \"tool_calls\": [\n",
    "      {\n",
    "        \"tool\": \"calculator\",\n",
    "        \"parameters\": {\"operation\": \"multiply\", \"x\": 123, \"y\": 456},\n",
    "      },\n",
    "      {\"tool\": \"weather_api\", \"parameters\": {\"city\": \"Paris\", \"country\": \"France\"}},\n",
    "    ],\n",
    "  },\n",
    "  {\"role\": \"tool\", \"tool_name\": \"calculator\", \"content\": \"56088\"},\n",
    "  {\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_name\": \"weather_api\",\n",
    "    \"content\": \"{'condition': 'rain', 'temperature': 15}\",\n",
    "  },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing a transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97064dfbeea843f4b3ce3923ca54bd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/everyday-conversations/train-00000-(â€¦):   0%|          | 0.00/946k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03f7473bb094a878d1b596aeac45876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/everyday-conversations/test-00000-o(â€¦):   0%|          | 0.00/52.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c0c9f1d0fd40eba5e9876f6e2021d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d173ac2abc412dba81be18725970f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceTB/smoltalk\", 'everyday-conversations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'content': 'Hi there', 'role': 'user'},\n",
       "  {'content': 'Hello! How can I help you today?', 'role': 'assistant'},\n",
       "  {'content': \"I'm looking for a beach resort for my next vacation. Can you recommend some popular ones?\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"Some popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'That sounds great. Are there any resorts in the Caribbean that are good for families?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Yes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"Okay, I'll look into those. Thanks for the recommendations!\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"You're welcome. I hope you find the perfect resort for your vacation.\",\n",
       "   'role': 'assistant'}],\n",
       " [{'content': 'Hi', 'role': 'user'},\n",
       "  {'content': 'Hello! How can I help you today?', 'role': 'assistant'},\n",
       "  {'content': \"I'm looking for career advice. I want to find a new job, but I'm not sure what I want to do.\",\n",
       "   'role': 'user'},\n",
       "  {'content': 'Career development can be challenging. What are your current skills and interests that might help narrow down some options?',\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"I have experience in marketing and enjoy working with people. I'm also interested in learning more about data analysis.\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"That's a great combination. You might consider roles like marketing analyst or business development, which combine people skills with data analysis. I can provide more information on those careers if you'd like.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'That sounds helpful. Can you also suggest some resources for learning data analysis?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Absolutely. Online courses like Coursera, edX, and LinkedIn Learning offer a wide range of data analysis courses. Additionally, you can explore professional networks like LinkedIn groups or attend industry events to learn from others in the field.',\n",
       "   'role': 'assistant'}]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['messages'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing functions can be used to convert the format to what the model expected\n",
    "def convert_to_chatml(example):\n",
    "  return {\n",
    "    \"message\": [\n",
    "      {\"role\": \"user\", \"content\": example['input']},\n",
    "      {\"role\": \"assistant\", \"content\": example['output']}\n",
    "    ]\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Supervised Fine-tuning\n",
    "\n",
    "SFT helps transform them into assistant-like models that can better understand and respond to user prompts. This is typically done by training on datasets of human-written conversations and instructions.\n",
    "\n",
    "SFT uses considerable resources, so only use it if:\n",
    "\n",
    "1. Other instruction-tuned models with well-crafted prompts do not meet the use case\n",
    "2. Need additional performance beyond what prompting can achieve\n",
    "3. Have a specific use case where the cost of using a large general-purpose model outweighs the cost of fine-tuning a smaller model\n",
    "4. Require specialized output formats or domain-specific knowledge that existing models struggle with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset should contain:\n",
    "\n",
    "1. Input prompt\n",
    "2. Expected model response\n",
    "3. Additional context or metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configuration\n",
    "\n",
    "Successful fine-tuning depends heavily on choosing the right training parameters. Below are the key parameters used\n",
    "\n",
    "<u>Training Duration</u>\n",
    "\n",
    "- `num_train_epochs`: Training duration\n",
    "- `max_steps`: Alternative to epochs, max number of training steps\n",
    "\n",
    "<u>Batch Size</u>\n",
    "\n",
    "Larger batches provide more stable gradients but require more memory\n",
    "\n",
    "- `per_device_train_batch_size`: Size of batch sent to each compute device (e.g GPU). Determines memory usage and training stability\n",
    "- `gradient_accumulation_steps`: When a single batch is split into smaller micro-batches and then recombined (summing) to accumulate the gradient for that batch\n",
    "\n",
    "<u>Learning Rate</u>\n",
    "\n",
    "Too high can cause instability\n",
    "\n",
    "- `learning_rate`: Controls size of weight updates\n",
    "- `warmup_ratio`: Portion of training used for learning rate warmup\n",
    "\n",
    "<u>Monitoring</u>\n",
    "\n",
    "- `logging_steps`: Frequency of metrics logged\n",
    "- `eval_steps`: How often to evaluate the validation data\n",
    "- `save_steps`: Frequency of model checkpoint saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dataset = load_dataset(\"HuggingFaceTB/smoltalk\", \"everyday-conversations\")\n",
    "\n",
    "model_checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the tokenizer into a chat-ready tokenizer, you need to provide a template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column([[{'content': 'Hi there', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': \"I'm looking for a beach resort for my next vacation. Can you recommend some popular ones?\", 'role': 'user'}, {'content': \"Some popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.\", 'role': 'assistant'}, {'content': 'That sounds great. Are there any resorts in the Caribbean that are good for families?', 'role': 'user'}, {'content': 'Yes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.', 'role': 'assistant'}, {'content': \"Okay, I'll look into those. Thanks for the recommendations!\", 'role': 'user'}, {'content': \"You're welcome. I hope you find the perfect resort for your vacation.\", 'role': 'assistant'}], [{'content': 'Hi', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': \"I'm looking for career advice. I want to find a new job, but I'm not sure what I want to do.\", 'role': 'user'}, {'content': 'Career development can be challenging. What are your current skills and interests that might help narrow down some options?', 'role': 'assistant'}, {'content': \"I have experience in marketing and enjoy working with people. I'm also interested in learning more about data analysis.\", 'role': 'user'}, {'content': \"That's a great combination. You might consider roles like marketing analyst or business development, which combine people skills with data analysis. I can provide more information on those careers if you'd like.\", 'role': 'assistant'}, {'content': 'That sounds helpful. Can you also suggest some resources for learning data analysis?', 'role': 'user'}, {'content': 'Absolutely. Online courses like Coursera, edX, and LinkedIn Learning offer a wide range of data analysis courses. Additionally, you can explore professional networks like LinkedIn groups or attend industry events to learn from others in the field.', 'role': 'assistant'}], [{'content': 'Hi', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': \"I'm going to the mall to do some window shopping. What's the point of window shopping if I'm not going to buy anything?\", 'role': 'user'}, {'content': 'Window shopping can be a fun way to browse and get inspiration for future purchases, or to simply enjoy looking at products and displays without feeling pressured to buy.', 'role': 'assistant'}, {'content': 'That makes sense. What are some basic rules I should follow while window shopping to be polite to the store staff?', 'role': 'user'}, {'content': 'Some basic rules include not touching or handling merchandise excessively, not blocking store entrances or aisles, and being respectful of staff and other customers.', 'role': 'assistant'}, {'content': \"Alright, I'll keep those in mind. Is it okay to take pictures of store displays or products?\", 'role': 'user'}, {'content': \"Yes, it's usually okay to take pictures of store displays or products for personal use, but it's always a good idea to ask store staff if it's allowed, especially if you're planning to share the photos online.\", 'role': 'assistant'}], [{'content': 'Hi there', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': \"I'm planning to cook dinner for my family tonight, do you have any easy recipe suggestions?\", 'role': 'user'}, {'content': 'Yes, I can suggest a simple pasta recipe with tomato sauce and vegetables. Would you like me to share the steps?', 'role': 'assistant'}, {'content': \"That sounds great, I'd love to hear the steps. And also, I want to gift some food to my neighbor, what's a good food gift idea?\", 'role': 'user'}, {'content': 'For the pasta recipe, just boil pasta, heat canned tomato sauce with sautÃ©ed vegetables, and combine. For a food gift, consider baking cookies or making a fruit basket â€“ both are easy and appreciated.', 'role': 'assistant'}], [{'content': 'Hi', 'role': 'user'}, {'content': 'Hello. How can I help you today?', 'role': 'assistant'}, {'content': \"What's the weather like today?\", 'role': 'user'}, {'content': \"The weather varies depending on your location. I can give you a general overview of the climate trends. There's been a rise in global temperatures due to climate change.\", 'role': 'assistant'}, {'content': 'What causes climate change?', 'role': 'user'}, {'content': 'Climate change is primarily caused by human activities, such as burning fossil fuels and deforestation, which release large amounts of greenhouse gases into the atmosphere.', 'role': 'assistant'}, {'content': 'How does climate change affect us?', 'role': 'user'}, {'content': 'Climate change has several impacts, including rising sea levels, more frequent natural disasters, and altered ecosystems, which can lead to food and water shortages, as well as increased risk of heat-related illnesses.', 'role': 'assistant'}], ...])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "# 1. Use the existing Instruct model tokenizer\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    "\n",
    "tokenizer.chat_template = instruct_tokenizer.chat_template\n",
    "# tokenizer.apply_chat_template(dataset['train']['messages'][0])\n",
    "print(instruct_tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|><|im_start|>userHi there<|im_end|><|im_start|>assistantHello! How can I help you today?<|im_end|><|im_start|>userI'm looking for a beach resort for my next vacation. Can you recommend some popular ones?<|im_end|><|im_start|>assistantSome popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.<|im_end|><|im_start|>userThat sounds great. Are there any resorts in the Caribbean that are good for families?<|im_end|><|im_start|>assistantYes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.<|im_end|><|im_start|>userOkay, I'll look into those. Thanks for the recommendations!<|im_end|><|im_start|>assistantYou're welcome. I hope you find the perfect resort for your vacation.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# 2. Manually define the template\n",
    "# Define ChatML template\n",
    "tokenizer.chat_template = (\n",
    "  \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}\"\n",
    "  \"{{ '<|im_start|>system You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>' }}\"\n",
    "  \"{% endif %}\"\n",
    "  \"{{'<|im_start|>' + message['role'] + '' + message['content'] + '<|im_end|>' + ''}}\"\n",
    "  \"{% endfor %}\"\n",
    "  \"{% if add_generation_prompt %}{{ '<|im_start|>assistant' }}{% endif %}\"\n",
    ")\n",
    "\n",
    "# Add special tokens to tokenizer\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]})\n",
    "\n",
    "# Resize model's embedding layer to match new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set the padding token to the original base model's\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Applying\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "  dataset['train']['messages'][0], \n",
    "  tokenize=True, \n",
    "  add_generation_prompt=False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(tokenized_chat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a1932d5f064c1782f7f27baf79adff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff086a9c39ae42e49b7be591643b2193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6ba87463e742db9e8c041814a8f8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3fb2b82ccd4cb4a0b8a296e61af2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = SFTConfig(\n",
    "  output_dir=\"./sft_output\",\n",
    "  max_steps=1000,\n",
    "  per_device_train_batch_size=4,\n",
    "  learning_rate=5e-5,\n",
    "  logging_steps=10,\n",
    "  save_steps=100,\n",
    "  eval_strategy=\"steps\",\n",
    "  eval_steps=50,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=dataset['train'],\n",
    "  eval_dataset=dataset['test'],\n",
    "  processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
      "2026/01/19 19:56:49 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/01/19 19:56:49 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/01/19 19:56:49 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/19 19:56:49 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade  -> 451aebb31d03, add metric step\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "2026/01/19 19:56:50 INFO alembic.runtime.migration: Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables\n",
      "2026/01/19 19:56:51 INFO alembic.runtime.migration: Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets\n",
      "2026/01/19 19:56:52 INFO alembic.runtime.migration: Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record\n",
      "2026/01/19 19:56:52 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table\n",
      "2026/01/19 19:56:52 INFO alembic.runtime.migration: Running upgrade bf29a5ff90ea -> 1bd49d398cd23, add secrets tables\n",
      "2026/01/19 19:56:52 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/19 19:56:52 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 04:31, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.111400</td>\n",
       "      <td>1.185057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.098000</td>\n",
       "      <td>1.099564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.031300</td>\n",
       "      <td>1.063168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.016600</td>\n",
       "      <td>1.040278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.004300</td>\n",
       "      <td>1.030836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.992700</td>\n",
       "      <td>1.018453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.971700</td>\n",
       "      <td>1.014258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.966000</td>\n",
       "      <td>1.009258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.981500</td>\n",
       "      <td>0.997688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.036000</td>\n",
       "      <td>0.987423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.953400</td>\n",
       "      <td>0.983420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.773800</td>\n",
       "      <td>0.986521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.783800</td>\n",
       "      <td>0.983246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.734300</td>\n",
       "      <td>0.985215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.825700</td>\n",
       "      <td>0.982574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.978496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.754800</td>\n",
       "      <td>0.978283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.793700</td>\n",
       "      <td>0.976531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.834100</td>\n",
       "      <td>0.975347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.758900</td>\n",
       "      <td>0.975293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.9477056078910827, metrics={'train_runtime': 281.7196, 'train_samples_per_second': 14.199, 'train_steps_per_second': 3.55, 'total_flos': 595026527745024.0, 'train_loss': 0.9477056078910827})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text\n",
    "\n",
    "2 methods:\n",
    "\n",
    "1. Manual conversion of text format to model\n",
    "2. Use the `pipeline` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Manual conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A solar panel is made up of multiple solar cells, each with a specific concentration of cells. When sunlight hits the solar panel, it causes the cells to release electrons, which are then transferred to the battery. This process is then repeated until the panel is fully charged.userIs a solar panel really that good?assistantYes, a solar panel is very efficient. It can convert about 15% of the sunlight it receives into usable energy, making it a great option for areas with minimal sunlight.userThat's good to know.assistantA solar panel is also important for renewable energy, as it helps reduce our reliance on fossil fuels and reduce greenhouse gas emissions.userI'd like to know more about the process of producing solar energy.assistantThe solar panel is made up of several components, including the photovoltaic cell, which converts sunlight into electricity, and the array, which is a group of solar panels that work together to generate electricity.userOkay, I think I understand now.assistantYou're welcome! I hope you find the solar panel helpful and exciting.userGreat, thank you for the help!assistantYou're welcome! I'm glad\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"Can you explain how a solar panel works?\"}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "  messages,\n",
    "  add_generation_prompt=True,\n",
    "  return_tensors='pt'\n",
    ").to(device)\n",
    "\n",
    "# Generate tokens\n",
    "outputs = model.generate(\n",
    "  input_ids,\n",
    "  max_new_tokens=256,\n",
    "  do_sample=True,\n",
    "  temperature=0.7,\n",
    "  top_k=50,\n",
    "  top_p=0.95\n",
    ")\n",
    "\n",
    "# Convert tokens back to text\n",
    "response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: `pipeline` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can make a cup of tea by placing a cup on a table, filling it with hot water, and pouring the water into a teapot.userWhat's the difference between a mug and a cup?assistantA mug is a larger and more common type of cup, often used for holding liquids such as tea and coffee. A cup is smaller and more portable, often used for drinks like sodas and juice.userAre there any specific types of cups for different types of drinks?assistantYes, there are several types of cups for different types of drinks. For example, a straw cup is a popular choice for drinking water, while a milk or orange water cup is used for making\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "  \"text-generation\",\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  device=0\n",
    ")\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful and concise assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": \"How do I make a cup of tea?\"}\n",
    "]\n",
    "\n",
    "output = generator(messages, max_new_tokens=150)\n",
    "print(output[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packing the dataset\n",
    "\n",
    "Allows multiple short examples to be packed into the same input sequence to maximise GPU utilisation during training\n",
    "\n",
    "- Set `packing=True` in `SFTConfig`\n",
    "- Might train for more epochs that expected when running with `max_steps`\n",
    "- Disable it for evaluation with `eval_packing=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(packing=True)\n",
    "trainer = SFTTrainer(model=model, train_dataset=dataset, args=training_args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom formatting function to combine fields into single input sequence\n",
    "def formatting_func(example):\n",
    "  \"\"\"Here the question and answer fields are combined into a single sequence\"\"\"\n",
    "  text = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n",
    "  return text\n",
    "\n",
    "\n",
    "training_args = SFTConfig(packing=True)\n",
    "trainer = SFTTrainer(\n",
    "  \"facebook/opt-350m\",\n",
    "  train_dataset=dataset,\n",
    "  args=training_args,\n",
    "  formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring training\n",
    "\n",
    "> Monitor both the loss values and the model's actual outputs during training. Sometimes loss can look good but the model outputs have unwanted responses\n",
    "\n",
    "Here are some additional qualitative evaluations to perform:\n",
    "\n",
    "1. Evaluate the model on a held-out test dataset\n",
    "2. Validate template adherence\n",
    "3. Test domain-specific knowledge retention\n",
    "4. Monitor real-world performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LoRA (Low-Rank Adaptation)\n",
    "\n",
    "Fine-tune LLMs with a smaller number of parameters. Adds and optmises small matrices to the attention weights.\n",
    "\n",
    "It freezes the pre-trained model weights and injects trainable rank decomposition matrices into the modelâ€™s layers. Reducing the number of trainable parameters while maintaining model performance\n",
    "\n",
    "LoRA works by adding pairs of rank decomposition matrices to transformer layers, typically focusing on attention weights. During _inference_, adapter weights can be merged with the base model, resulting in no latency overhead\n",
    "\n",
    "âœ… Advantages\n",
    "\n",
    "- __Memory efficient:__ Only adapter parameters are stored in GPU memory, base model weights are frozen\n",
    "- __Training Features__\n",
    "- __Adapter Management__\n",
    "\n",
    "## Parameter-efficient fine-tuning (PEFT)\n",
    "\n",
    "Library to efficiently load and switch between different PEFT methods. Adapters are weights that are not attached to the original base model, but are added during the LoRA process.\n",
    "\n",
    "- `load_adapter()`: Loads adapter weights\n",
    "- `set_adapter()`: Adds the active adapter weights to the model\n",
    "- `unload()`: Returns to base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![lora configuration](./assets/lora_config.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from peft import PeftModel, PeftConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from dotenv import dotenv_values\n",
    "import mlflow\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow configurations\n",
    "env_config = dotenv_values(\".env\")\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = env_config[\"MLFLOW_USER\"]\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = env_config[\"MLFLOW_PASSWORD\"]\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://127.0.0.1:9000\"\n",
    "os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = \"true\"\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow/5', creation_time=1768980702967, experiment_id='5', last_update_time=1768980702967, lifecycle_stage='active', name='hf-lora-experiment', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"hf-lora-experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceTB/smoltalk\", \"everyday-conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic loading of model with PEFT weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb8f963024447ccabdee30e52716d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/416 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b141a25080b748679e09973cef6bb504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec4170991ec45a391d9a24af59f40f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    }
   ],
   "source": [
    "config = PeftConfig.from_pretrained(\"ybelkada/opt-350m-lora\")\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "lora_model = PeftModel.from_pretrained(model, \"ybelkada/opt-350m-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning with SFTTrainer and LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 0,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "loading configuration file generation_config.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside HuggingFaceTB/SmolLM2-135M.\n",
      "loading file vocab.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/vocab.json\n",
      "loading file merges.txt from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/merges.txt\n",
      "loading file tokenizer.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading file vocab.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/12fd25f77366fa6b3b4b768ec3050bf629380bac/vocab.json\n",
      "loading file merges.txt from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/12fd25f77366fa6b3b4b768ec3050bf629380bac/merges.txt\n",
      "loading file tokenizer.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/12fd25f77366fa6b3b4b768ec3050bf629380bac/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/12fd25f77366fa6b3b4b768ec3050bf629380bac/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/12fd25f77366fa6b3b4b768ec3050bf629380bac/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    "tokenizer.chat_template = instruct_tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "rank_dimension = 6            # r: smaller = more compression but less expressive\n",
    "lora_alpha = 8                # lora_alpha: higher = stronger adaptation (usually 2x rank value)\n",
    "lora_dropout = 0.05           # lora_dropout: dropout probability for lora layers\n",
    "bias = \"none\"                 # bias: whether to inlcude bias in layers specified\n",
    "target_modules = \"all-linear\" # target_modules: which layers to apply Lora to\n",
    "task_type = \"CAUSAL_LM\"       # task_type: model architecture\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "  r=rank_dimension,\n",
    "  lora_alpha=lora_alpha,\n",
    "  lora_dropout=lora_dropout,\n",
    "  bias=bias,\n",
    "  target_modules=target_modules,\n",
    "  task_type=task_type,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No output directory specified, defaulting to 'trainer_output'. To change this behavior, specify --output_dir when creating TrainingArguments.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n",
      "The following columns in the Training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages, full_topic. If messages, full_topic are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2,260\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 1,831,680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 03:13, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.611500</td>\n",
       "      <td>2.623662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.175300</td>\n",
       "      <td>2.185436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.806700</td>\n",
       "      <td>1.797163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.562000</td>\n",
       "      <td>1.578676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.438000</td>\n",
       "      <td>1.458313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.369000</td>\n",
       "      <td>1.403291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.366000</td>\n",
       "      <td>1.375881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.335100</td>\n",
       "      <td>1.361207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.328200</td>\n",
       "      <td>1.353724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.375200</td>\n",
       "      <td>1.351394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages, full_topic. If messages, full_topic are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 8\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages, full_topic. If messages, full_topic are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to trainer_output/checkpoint-100\n",
      "loading configuration file config.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 0,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "chat template saved in trainer_output/checkpoint-100/chat_template.jinja\n",
      "tokenizer config file saved in trainer_output/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in trainer_output/checkpoint-100/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages, full_topic. If messages, full_topic are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 8\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages, full_topic. If messages, full_topic are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to trainer_output/checkpoint-200\n",
      "loading configuration file config.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 0,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "chat template saved in trainer_output/checkpoint-200/chat_template.jinja\n",
      "tokenizer config file saved in trainer_output/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in trainer_output/checkpoint-200/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages, full_topic. If messages, full_topic are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 8\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages, full_topic. If messages, full_topic are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to trainer_output/checkpoint-300\n",
      "loading configuration file config.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 0,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "chat template saved in trainer_output/checkpoint-300/chat_template.jinja\n",
      "tokenizer config file saved in trainer_output/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in trainer_output/checkpoint-300/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages, full_topic. If messages, full_topic are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 8\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages, full_topic. If messages, full_topic are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to trainer_output/checkpoint-400\n",
      "loading configuration file config.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 0,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "chat template saved in trainer_output/checkpoint-400/chat_template.jinja\n",
      "tokenizer config file saved in trainer_output/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in trainer_output/checkpoint-400/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages, full_topic. If messages, full_topic are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 8\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: messages, full_topic. If messages, full_topic are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 119\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to trainer_output/checkpoint-500\n",
      "loading configuration file config.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 0,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "chat template saved in trainer_output/checkpoint-500/chat_template.jinja\n",
      "tokenizer config file saved in trainer_output/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trainer_output/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./models/lora_smolLM\n",
      "loading configuration file config.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 0,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "chat template saved in ./models/lora_smolLM/chat_template.jinja\n",
      "tokenizer config file saved in ./models/lora_smolLM/tokenizer_config.json\n",
      "Special tokens file saved in ./models/lora_smolLM/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bemused-snake-698 at: http://127.0.0.1:5000/#/experiments/5/runs/e72e0396356547988720246ce1864b42\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/5\n"
     ]
    }
   ],
   "source": [
    "training_args = SFTConfig(\n",
    "  max_steps=500,\n",
    "  per_device_train_batch_size=4,\n",
    "  learning_rate=5e-5,\n",
    "  logging_steps=10,\n",
    "  save_steps=100,\n",
    "  eval_strategy=\"steps\",\n",
    "  eval_steps=50,\n",
    ")\n",
    "\n",
    "mlflow.transformers.autolog()\n",
    "\n",
    "with mlflow.start_run():\n",
    "  trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    processing_class=tokenizer\n",
    "  )\n",
    "\n",
    "  trainer.train()\n",
    "\n",
    "  trainer.save_model(\"./models/lora_smolLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd28d71760f1464c8ca55b7f57ca2e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab88e0c5073546c788e85eaf44fbf77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464ddeadaa7142818f8906938645cff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Minimartzz/lora-smolLM/commit/6baaa7f4d88fe4901c3299f2d960be13c3aa3371', commit_message='Upload tokenizer', commit_description='', oid='6baaa7f4d88fe4901c3299f2d960be13c3aa3371', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Minimartzz/lora-smolLM', endpoint='https://huggingface.co', repo_type='model', repo_id='Minimartzz/lora-smolLM'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push trained model to hub\n",
    "model.push_to_hub(\"Minimartzz/lora-smolLM\")\n",
    "tokenizer.push_to_hub(\"Minimartzz/lora-smolLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA Adapters\n",
    "model.save_pretrained(\"./models/lora_smolLM_adapter\")\n",
    "peft_config.save_pretrained(\"./models/lora_smolLM_config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging LoRA adapters\n",
    "\n",
    "After training, merging the adapter weights with the base model is commonly done for easier deployment. It creates a single model with combined weights, eliminating the need to load adapters separately during inference\n",
    "\n",
    "ðŸš¨ CRITICAL: Ensure that sufficient memory in GPU/ CPU is available before loading\n",
    "\n",
    "Use `device_map=\"auto\"` to attach the weights to the correct device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 0,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/model.safetensors\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "loading configuration file generation_config.json from cache at /home/minimartzz/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside HuggingFaceTB/SmolLM2-135M.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "adapter_path = \"./models/lora_smolLM\"\n",
    "config = PeftConfig.from_pretrained(adapter_path)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "  config.base_model_name_or_path, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "  base_model, adapter_path, torch_dtype=torch.float16, local_files_only=True\n",
    ")\n",
    "\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"base_model_name\")\n",
    "merged_model.save_pretrained(\"path/to/save/merged_model\")\n",
    "tokenizer.save_pretrained(\"path/to/save/merged_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation\n",
    "\n",
    "Always evaluate models on standard benchmarks to measure performance. Below are common benchmarks\n",
    "\n",
    "- __Automatic Benchmarks__ - Standardized tools for evaluating language models across different tasks and capabilities. Good starting point, but only one component in evaluation\n",
    "  * Consists of curated datasets with predefined tasks and evaluation metrics\n",
    "  * Assess various aspects of model capabilities (e.g basic language understanding)\n",
    "  * Consistent comparison across models\n",
    "- __General Knowledge Benchmarks__ - Tests answering questions\n",
    "  * _MMLU_: Tests knowledge across 57 subjects, from science to humanities\n",
    "  * _TruthfulQA_: Evaluates a modelâ€™s tendency to reproduce common misconceptions\n",
    "- __Reasoning Benchmarks__ - Tests complex reasoning tasks. Assess analytical capabilities\n",
    "  * _BBH_: Tests logical thinking and planning\n",
    "  * _GSM8K_: Targets mathematical problem-solving\n",
    "- __Language Understanding__ - General understanding on how language is perceived\n",
    "  * _HELM_: Language processing capabilities on aspects like commonsense, world knowledge, and reasoning\n",
    "- __Domain-Specific Benchmarks__ - Benchmarks that focus on specific tasks\n",
    "  * _MATH_: 12,500 problems from mathematics competitions. Requires multi-step reasoning, and the generation of step-by-step solutions\n",
    "  * _HumanEval_: Coding-focused evaluation dataset consisting of 164 programming problems\n",
    "  * _Alpaca Eval_: Assess the quality of instruction-following language models. Uses GPT-4 as a judge to evaluate model outputs across various dimensions including helpfulness, honesty, and harmlessness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom evaluation\n",
    "\n",
    "Developing a more comprehensive benchmarking apporach:\n",
    "\n",
    "1. Start with relevant standard benchmarks to establish a baseline\n",
    "2. Identify the specific requirements and challenges of your use case\n",
    "3. Develop custom evaluation datasets that reflect your actual use case (e.g real user queries from your domain, common edge cases)\n",
    "4. Consider implementing a multi-layered evaluation strategy\n",
    "\n",
    "ðŸ§° Tools to use: [lighteval](https://github.com/huggingface/lighteval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "lighteval accelerate \\\n",
    "  \"pretrained=your-model-name\" \\\n",
    "  \"mmlu|anatomy|0|0\" \\\n",
    "  \"mmlu|high_school_biology|0|0\" \\\n",
    "  \"mmlu|high_school_chemistry|0|0\" \\\n",
    "  \"mmlu|professional_medicine|0|0\" \\\n",
    "  --max_samples 40 \\\n",
    "  --batch_size 1 \\\n",
    "  --output_path \"./results\" \\\n",
    "  --save_generations true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
