{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing Models and Tokenizers on HuggingFace Hub\n",
    "\n",
    "Learning how to save and share trained models on HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 08/12/2025   | Martin | Created   | Notebook created for model sharing on HF Hub | \n",
    "| 09/12/2025   | Martin | Update   | Completed Ch4. Creating repos, uploading models files, and model cards | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Sharing Models](#sharing-models)\n",
    "* [Saving Model File](#saving-model-files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "- Each model is hosted as a Git repository\n",
    "- Sharing models on the Hub automatically deploys a hosted Inference API for the model i.e anyone in the community can test and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing using pipeline - Ensure the right pipeline task as stated on model card\n",
    "from transformers import pipeline\n",
    "\n",
    "camembert_fill_mask = pipeline(\"fill-mask\", model=\"camembert-base\")\n",
    "results = camembert_fill_mask(\"Le camembert est <mask> :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing using specific model card\n",
    "from transformers import CamembertTokenizer, CamembertForMaskedLM\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import using Auto* class - Recommended\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing Models\n",
    "\n",
    "- Repository name will be what was specified as the output directory\n",
    "  - Can be changed with `hub_model_id` argument\n",
    "- Save frequency based on the `save_strategy` argument\n",
    "- Final `trainer.push_to_hub()` to save final iteration of model\n",
    "\n",
    "<u>3 methods to sharing</u>\n",
    "\n",
    "1. `push_to_hub` API\n",
    "2. `huggingface_hub` Python library\n",
    "3. Web interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Push to hub\n",
    "\n",
    "- Requires authentication tokens to indicate user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8dcc36239d9453290591e3fde496806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Login to HF hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  \"bert-finetuned-mrpc\",\n",
    "  save_strategy=\"epoch\",\n",
    "  push_to_hub=True # >>: Automatically pushes to a new repository in the Hub\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"dummy-model\")\n",
    "tokenizer.push_to_hub(\"dummy-model\", organization=\"huggingface\", use_auth_token=\"<TOKEN>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. huggingface_hub\n",
    "\n",
    "Python library that offers tools to interact with the Hub.\n",
    "\n",
    "- Also requires API token in cache to work\n",
    "- Get information about repositories on the hub and managing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/Minimartzz/dummy-model', endpoint='https://huggingface.co', repo_type='model', repo_id='Minimartzz/dummy-model')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "create_repo(\n",
    "  \"dummy-model\",\n",
    "  # organization=\"martz\",\n",
    "  # private=False,\n",
    "  # token=\"<HF User Token>\",\n",
    "  # repo_type=\"dataset or space\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of common tasks performable through the API\n",
    "from huggingface_hub import (\n",
    "  # User management\n",
    "  login,\n",
    "  logout,\n",
    "  whoami,\n",
    "\n",
    "  # Repository creation and management\n",
    "  create_repo,\n",
    "  delete_repo,\n",
    "  update_repo_visibility,\n",
    "\n",
    "  # And some methods to retrieve/change information about the content\n",
    "  list_models,\n",
    "  list_datasets,\n",
    "  # list_metrics, # >>: Deprecated\n",
    "  list_repo_files,\n",
    "  upload_file,\n",
    "  delete_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Web interface\n",
    "\n",
    "Most features here are also available on the web interface. Creating a repo, updating README, adding model cards, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model Files\n",
    "\n",
    "system to manage files on the Hugging Face Hub is based on __git__ for regular files, and __git-lfs__ (which stands for Git Large File Storage) for larger files\n",
    "\n",
    "1. `upload_file` - Does not require git or git-lfs, but has a size limit of 5GB\n",
    "2. `Repository` class - Abstracts away the git commands into a Pythonic class\n",
    "3. `git` based - Use the git CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_file function\n",
    "from huggingface_hub import upload_file\n",
    "\n",
    "upload_file(\n",
    "  '<path_to_file>/config.json',\n",
    "  path_in_repo='config.json',\n",
    "  repo_id=\"<namespace>/dummy-model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository class\n",
    "from huggingface_hub import Repository\n",
    "\n",
    "repo = Repository(\"<path_to_dummy_folder>\", clone_from=\"<namespace>/dummy model\")\n",
    "\n",
    "# Commands\n",
    "repo.git_pull()\n",
    "repo.git_add()\n",
    "repo.git_commit()\n",
    "repo.git_push()\n",
    "repo.git_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a push\n",
    "repo.git_add()\n",
    "repo.git_commit(\"Add model and tokenizer files\")\n",
    "repo.git_push()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Cards\n",
    "\n",
    "The central definition of a model. Created to ensure reusability and reproducibility of results\n",
    "\n",
    "- Document training and evaluation process\n",
    "- Provide sufficient information about the data, including preprocessing and postprocessing\n",
    "- Any limitations, biases, and context that model cannot cover\n",
    "\n",
    "<u>Recommended Sections</u>\n",
    "\n",
    "- High-level Overview of Model use\n",
    "- Model description\n",
    "- Intended uses & limitations\n",
    "- How to use\n",
    "- Limitations and bias\n",
    "- Training data\n",
    "- Training procedure\n",
    "- Evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description\n",
    "\n",
    "- Architecture\n",
    "- Version\n",
    "- Original paper (if based on one)\n",
    "- Author\n",
    "- Copyright\n",
    "- General info about training procedure, parameters and disclaimers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intended uses & limitations\n",
    "\n",
    "- Languages, fields and domains it's applicable to\n",
    "- Areas that are out-of-scope for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "- Usage of the model\n",
    "- Tokeniser\n",
    "- Other code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "- Which dataset(s) the model was trained on + description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procedure\n",
    "\n",
    "- Relevant aspects of training that are useful from a reproducibility perspective\n",
    "- Preprocessing and postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable and metrics\n",
    "\n",
    "- Metrics you use for evaluation\n",
    "- Metrics should be based on the intended users and use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation results\n",
    "\n",
    "- How well the model performs on the evaluation dataset\n",
    "- Provide decision threshold (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
