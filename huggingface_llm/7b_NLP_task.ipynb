{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tasks (Part 2)\n",
    "\n",
    "Question answering task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 30/12/2025   | Martin | Created   | Notebook for Question-answering task | \n",
    "| 08/01/2025   | Martin | Update   | Completed Question-answering task | \n",
    "| 09/01/2025   | Martin | Update   | Added note on data collators | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [A Note on Data Collators](#note-on-data-collators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Extracative Question Answering: Finding the answer of a question from provided text\n",
    "\n",
    "- Dataset: Question and Answers from `SQuAD`\n",
    "- Training only has a single answer per question, but evaluation has multiple answers\n",
    "- Model is trained to predict the __start and end__ logit per token in the input context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"squad\")\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answer:  {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Context: \", raw_dataset[\"train\"][0][\"context\"])\n",
    "print(\"Question: \", raw_dataset[\"train\"][0][\"question\"])\n",
    "print(\"Answer: \", raw_dataset[\"train\"][0][\"answers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tokenizer. Sentences will be in the format:\n",
    "\n",
    "> [CLS] question [SEP] context [SEP]\n",
    "\n",
    "Truncate the context to limit each data point length. Will create multiple samples from a single question-answer pair. Some tokens will overflow into the next example.\n",
    "\n",
    "- Will create additional entries for those that overflow\n",
    "- For those questions where the answer is not contained, predict 0 for start and end logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building ' s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n"
     ]
    }
   ],
   "source": [
    "context = raw_dataset[\"train\"][0][\"context\"]\n",
    "question = raw_dataset[\"train\"][0][\"question\"]\n",
    "\n",
    "inputs = tokenizer(question, context)\n",
    "print(tokenizer.decode(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building ' s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]\n",
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]\n",
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]\n",
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "  question,\n",
    "  context,\n",
    "  max_length=100,                 # Maximum length of context\n",
    "  truncation='only_second',       # Context is in the second position of example\n",
    "  stride=50,                      # Number of tokens to overlap from previous truncation\n",
    "  return_overflowing_tokens=True, # Indicate to take overflowing tokens\n",
    "  return_offsets_mapping=True\n",
    ")\n",
    "\n",
    "for ids in inputs['input_ids']:\n",
    "  print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': [[101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 102], [101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 102], [101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 102], [101, 1706, 2292, 1225, 1103, 6567, 2090, 9273, 2845, 1107, 8109, 1107, 10111, 20500, 1699, 136, 102, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "  raw_dataset[\"train\"][2:6][\"question\"],\n",
    "  raw_dataset[\"train\"][2:6][\"context\"],\n",
    "  max_length=100,\n",
    "  truncation=\"only_second\",\n",
    "  stride=50,\n",
    "  return_overflowing_tokens=True,\n",
    "  return_offsets_mapping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "token_type_ids\n",
      "attention_mask\n",
      "offset_mapping\n",
      "overflow_to_sample_mapping\n"
     ]
    }
   ],
   "source": [
    "for k in inputs.keys():\n",
    "  print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] The Basilica of the Sacred heart at Notre Dame is beside to which structure? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building ' s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]\n",
      "0\n",
      "[CLS] The Basilica of the Sacred heart at Notre Dame is beside to which structure? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]\n",
      "0\n",
      "[CLS] The Basilica of the Sacred heart at Notre Dame is beside to which structure? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]\n",
      "0\n",
      "[CLS] The Basilica of the Sacred heart at Notre Dame is beside to which structure? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n",
      "0\n",
      "[CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building ' s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]\n",
      "1\n",
      "[CLS] What is the Grotto at Notre Dame? [SEP] it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette So [SEP]\n",
      "1\n",
      "[CLS] What is the Grotto at Notre Dame? [SEP] Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of [SEP]\n",
      "1\n",
      "[CLS] What is the Grotto at Notre Dame? [SEP] where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n",
      "1\n",
      "[CLS] What sits on top of the Main Building at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building ' s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the [SEP]\n",
      "2\n",
      "[CLS] What sits on top of the Main Building at Notre Dame? [SEP] and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint [SEP]\n",
      "2\n",
      "[CLS] What sits on top of the Main Building at Notre Dame? [SEP] the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a [SEP]\n",
      "2\n",
      "[CLS] What sits on top of the Main Building at Notre Dame? [SEP]to at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n",
      "2\n",
      "[CLS] When did the Scholastic Magazine of Notre dame begin publishing? [SEP] As at most other universities, Notre Dame ' s students run a number of news media outlets. The nine student - run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one - page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Ju [SEP]\n",
      "3\n",
      "[CLS] When did the Scholastic Magazine of Notre dame begin publishing? [SEP] television station, and several magazines and journals. Begun as a one - page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The [SEP]\n",
      "3\n",
      "[CLS] When did the Scholastic Magazine of Notre dame begin publishing? [SEP] be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary ' s College. Unlike Scholastic and The Dome [SEP]\n",
      "3\n",
      "[CLS] When did the Scholastic Magazine of Notre dame begin publishing? [SEP] The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary ' s College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began [SEP]\n",
      "3\n",
      "[CLS] When did the Scholastic Magazine of Notre dame begin publishing? [SEP] both Notre Dame and Saint Mary ' s College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the [SEP]\n",
      "3\n",
      "[CLS] When did the Scholastic Magazine of Notre dame begin publishing? [SEP] editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer ; however, all three are distributed to all students. Finally, in Spring 2008 [SEP]\n",
      "3\n",
      "[CLS] When did the Scholastic Magazine of Notre dame begin publishing? [SEP], in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer ; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut. [SEP]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for ids, mapping in zip(inputs['input_ids'], inputs['overflow_to_sample_mapping']):\n",
    "  print(tokenizer.decode(ids))\n",
    "  print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels will be of the format: `(start_position, end_position)`\n",
    "\n",
    "- Which are tuples of 2 integers representing the span of characters inside the original context\n",
    "- If it appears before the context, then return `(0, 0)`\n",
    "- Else loop to find the first and last token in the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': ['the Main Building'], 'answer_start': [279]},\n",
       " {'text': ['a Marian place of prayer and reflection'], 'answer_start': [381]},\n",
       " {'text': ['a golden statue of the Virgin Mary'], 'answer_start': [92]},\n",
       " {'text': ['September 1876'], 'answer_start': [248]}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3],\n",
       " [83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],\n",
       " [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Searching for answers within the training data\n",
    "answers = raw_dataset['train'][2:6]['answers']\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for i, offset in enumerate(inputs['offset_mapping']):\n",
    "  answers_offset = inputs['overflow_to_sample_mapping'][i]\n",
    "  answer = answers[answers_offset]\n",
    "  start_char = answer['answer_start'][0]\n",
    "  end_char = answer['answer_start'][0] + len(answer['text'][0])\n",
    "  sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "  # Find the start and end of context\n",
    "  cont_idx = 0\n",
    "  while sequence_ids[cont_idx] != 1:\n",
    "    cont_idx += 1\n",
    "  start_cont_idx = cont_idx\n",
    "  while sequence_ids[cont_idx] == 1:\n",
    "    cont_idx += 1\n",
    "  end_cont_idx = cont_idx - 1\n",
    "\n",
    "  # Check if the answer is inside the context or not\n",
    "  if offset[start_cont_idx][0] > start_char or offset[end_cont_idx][1] < end_char:\n",
    "    start_positions.append(0)\n",
    "    end_positions.append(0)\n",
    "  else:\n",
    "    idx = start_cont_idx\n",
    "    while idx <= end_cont_idx and offset[idx][0] <= start_char:\n",
    "      idx += 1\n",
    "    start_positions.append(idx - 1)\n",
    "\n",
    "    idx = end_cont_idx\n",
    "    while idx >= start_cont_idx and offset[idx][1] >= end_char:\n",
    "      idx -= 1\n",
    "    end_positions.append(idx + 1)\n",
    "\n",
    "inputs['overflow_to_sample_mapping'], start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: the Main Building | Labels give: the Main Building\n"
     ]
    }
   ],
   "source": [
    "# Check results to verify apporach\n",
    "idx = 0\n",
    "sample_idx = inputs['overflow_to_sample_mapping'][idx]\n",
    "answer = answers[sample_idx]['text'][0]\n",
    "\n",
    "start = start_positions[idx]\n",
    "end = end_positions[idx]\n",
    "labeled_answer = tokenizer.decode(inputs['input_ids'][idx][start : end + 1])\n",
    "print(f\"Theoretical answer: {answer} | Labels give: {labeled_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['overflow_to_sample_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 384\n",
    "STRIDE = 128\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "  questions = [q.strip() for q in examples['question']]\n",
    "  inputs = tokenizer(\n",
    "    questions,\n",
    "    examples['context'],\n",
    "    max_length=MAX_LEN,\n",
    "    truncation=\"only_second\",\n",
    "    stride=STRIDE,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\"\n",
    "  )\n",
    "\n",
    "  offset_mapping = inputs.pop('offset_mapping')\n",
    "  sample_map = inputs.pop('overflow_to_sample_mapping')\n",
    "  answers = examples['answers']\n",
    "  start_positions = []\n",
    "  end_positions = []\n",
    "\n",
    "  for i, offset in enumerate(offset_mapping):\n",
    "    sample_idx = sample_map[i]\n",
    "    answer = answers[sample_idx]\n",
    "    start_char = answer['answer_start'][0]\n",
    "    end_char = answer['answer_start'][0] + len(answer['text'][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of context\n",
    "    cont_idx = 0\n",
    "    while sequence_ids[cont_idx] != 1:\n",
    "      cont_idx += 1\n",
    "    start_cont_idx = cont_idx\n",
    "    while sequence_ids[cont_idx] == 1:\n",
    "      cont_idx += 1\n",
    "    end_cont_idx = cont_idx - 1\n",
    "\n",
    "    # Check if the answer is inside the context or not\n",
    "    if offset[start_cont_idx][0] > start_char or offset[end_cont_idx][1] < end_char:\n",
    "      start_positions.append(0)\n",
    "      end_positions.append(0)\n",
    "    else:\n",
    "      idx = start_cont_idx\n",
    "      while idx <= end_cont_idx and offset[idx][0] <= start_char:\n",
    "        idx += 1\n",
    "      start_positions.append(idx - 1)\n",
    "\n",
    "      idx = end_cont_idx\n",
    "      while idx >= start_cont_idx and offset[idx][1] >= end_char:\n",
    "        idx -= 1\n",
    "      end_positions.append(idx + 1)\n",
    "  \n",
    "  inputs['start_positions'] = start_positions\n",
    "  inputs['end_positions'] = end_positions\n",
    "\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88729, 87599)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = raw_dataset['train'].map(\n",
    "  preprocess_training_examples,\n",
    "  batched=True,\n",
    "  remove_columns=raw_dataset['train'].column_names\n",
    ")\n",
    "\n",
    "len(train_dataset), len(raw_dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing validation data\n",
    "\n",
    "Ignore \"answers\" from the question by setting the offset for questions to be `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "  questions = [q.strip() for q in examples[\"question\"]]\n",
    "  inputs = tokenizer(\n",
    "    questions,\n",
    "    examples[\"context\"],\n",
    "    max_length=MAX_LEN,\n",
    "    truncation=\"only_second\",\n",
    "    stride=STRIDE,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    "  )\n",
    "\n",
    "  sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "  example_ids = []\n",
    "\n",
    "  for i in range(len(inputs[\"input_ids\"])):\n",
    "    sample_idx = sample_map[i]\n",
    "    example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "    offset = inputs[\"offset_mapping\"][i]\n",
    "    inputs[\"offset_mapping\"][i] = [\n",
    "      o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "    ]\n",
    "\n",
    "  inputs[\"example_id\"] = example_ids\n",
    "\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a721229275a455dbaad49ef171991b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10570, 10822)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = raw_dataset[\"validation\"].map(\n",
    "  preprocess_validation_examples,\n",
    "  batched=True,\n",
    "  remove_columns=raw_dataset[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_dataset[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning with Trainer\n",
    "\n",
    "- No data collator to define since the sequence length is padded to the max\n",
    "- Post-process the model predictions into spans of text in the original examples\n",
    "- Use `compute_metric()` to measure performance\n",
    "- Model outputs __logits__ for the start and end position of the answer for each input\n",
    "  * Mask all words that are not within the context of the current input\n",
    "  * Only score the top $n$ start and end tokens\n",
    "  * Take product of logits between both tokens (log scale, so sum instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample evaluation\n",
    "\n",
    "Sample of evaluation using another model - `distilbert-base-cased-distilled-squad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_eval_set = raw_dataset['validation'].select(range(100))\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "eval_set = small_eval_set.map(\n",
    "  preprocess_validation_examples,\n",
    "  batched=True,\n",
    "  remove_columns=raw_dataset['validation'].column_names\n",
    ")\n",
    "\n",
    "# Reset tokenizer back to original\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_set_for_model['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Move data to GPU\n",
    "eval_set_for_model = eval_set.remove_columns(['example_id', 'offset_mapping'])\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "eval_set_for_model.set_format('torch', device=device)\n",
    "\n",
    "# Convert Columns to tensors and move model to GPU\n",
    "batch = {k: eval_set_for_model[k][:] for k in eval_set_for_model.column_names}\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n",
    "  device\n",
    ")\n",
    "\n",
    "# Make predictions with trained model\n",
    "with torch.no_grad():\n",
    "  outputs = trained_model(**batch)\n",
    "\n",
    "# Get the start and end logit positions\n",
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits = outputs.end_logits.cpu().numpy()\n",
    "\n",
    "# Map each small_eval_set to the corresponding entry in the eval_set\n",
    "example_to_features = collections.defaultdict(list)\n",
    "for idx, feature in enumerate(eval_set):\n",
    "  example_to_features[feature['example_id']].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all examples and their associated features. Ignore positions that:\n",
    "\n",
    "- Answers that are not in the context\n",
    "- Answers with negative length\n",
    "- Answer that is too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BEST = 20      # Take the top 20 logit scores\n",
    "MAX_ANS_LEN = 30 # Maximum answer length\n",
    "predicted_answers = []\n",
    "\n",
    "for example in small_eval_set:\n",
    "  example_id = example['id']\n",
    "  context = example['context']\n",
    "  answers = [] # Multiple answers since each question is broken up into many features\n",
    "\n",
    "  for feature_index in example_to_features[example_id]:\n",
    "    start_logit = start_logits[feature_index]\n",
    "    end_logit = end_logits[feature_index]\n",
    "    offsets = eval_set['offset_mapping'][feature_index]\n",
    "\n",
    "    # Get the index of the top 20 logit values\n",
    "    start_indexes = np.argsort(start_logit)[-1:-N_BEST-1:-1].tolist()\n",
    "    end_indexes = np.argsort(end_logit)[-1:-N_BEST-1:-1].tolist()\n",
    "    for start_index in start_indexes:\n",
    "      for end_index in end_indexes:\n",
    "        # Check 1: Ignore if index is not in context\n",
    "        if offsets[start_index] is None or offsets[end_index] is None:\n",
    "          continue\n",
    "\n",
    "        # Check 2: Skip answers that are negative length or above max length\n",
    "        if (\n",
    "          end_index < start_index\n",
    "          or end_index - start_index + 1 > MAX_ANS_LEN\n",
    "        ):\n",
    "          continue\n",
    "        \n",
    "        answers.append(\n",
    "          {\n",
    "            \"text\": context[offsets[start_index][0]:offsets[end_index][1]],\n",
    "            \"logit_score\": start_logit[start_index] + end_logit[end_index]\n",
    "          }\n",
    "        )\n",
    "  \n",
    "  best_answer = max(answers, key=lambda x: x['logit_score'])\n",
    "  predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56be4db0acb8001400a502ed', 'prediction_text': 'Carolina Panthers'},\n",
       " {'id': '56be4db0acb8001400a502ee',\n",
       "  'prediction_text': \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\"},\n",
       " {'id': '56be4db0acb8001400a502ef', 'prediction_text': 'Carolina Panthers'},\n",
       " {'id': '56be4db0acb8001400a502f0', 'prediction_text': 'gold'},\n",
       " {'id': '56be8e613aeaaa14008c90d1', 'prediction_text': 'golden anniversary'},\n",
       " {'id': '56be8e613aeaaa14008c90d2', 'prediction_text': 'February 7, 2016'},\n",
       " {'id': '56be8e613aeaaa14008c90d3',\n",
       "  'prediction_text': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference'},\n",
       " {'id': '56bea9923aeaaa14008c91b9', 'prediction_text': 'golden anniversary'},\n",
       " {'id': '56bea9923aeaaa14008c91ba',\n",
       "  'prediction_text': 'American Football Conference'}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_answers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc57fabc62f4ea1880e02c1a40bf111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ee0088c9614f4d8f6d068d1d707927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}\n",
      "{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the answers\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "# Predicted answers must be in the format\n",
    "# id: example_id\n",
    "# answers: text answer\n",
    "theoretical_answers = [\n",
    "  {\n",
    "    \"id\": ex['id'],\n",
    "    \"answers\": ex['answers']\n",
    "  } for ex in small_eval_set\n",
    "]\n",
    "\n",
    "print(theoretical_answers[0])\n",
    "print(predicted_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "\n",
    "`compute_metrics()` function requires additional information for offsets and example ids. So it's only present at the end of the training loop to check for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d6a8b31b1a413d9079710c58ba9be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "  # Map example_id to features\n",
    "  example_to_features = collections.defaultdict(list)\n",
    "  for idx, feature in enumerate(features):\n",
    "    example_to_features[feature['example_id']].append(idx)\n",
    "\n",
    "  predicted_answers = []\n",
    "  for example in tqdm(examples):\n",
    "    example_id = example['id']\n",
    "    context = example['context']\n",
    "    answers = [] # Multiple answers since each question is broken up into many features\n",
    "\n",
    "    for feature_index in example_to_features[example_id]:\n",
    "      start_logit = start_logits[feature_index]\n",
    "      end_logit = end_logits[feature_index]\n",
    "      offsets = eval_set['offset_mapping'][feature_index]\n",
    "\n",
    "      # Get the index of the top 20 logit values\n",
    "      start_indexes = np.argsort(start_logit)[-1:-N_BEST-1:-1].tolist()\n",
    "      end_indexes = np.argsort(end_logit)[-1:-N_BEST-1:-1].tolist()\n",
    "      for start_index in start_indexes:\n",
    "        for end_index in end_indexes:\n",
    "          # Check 1: Ignore if index is not in context\n",
    "          if offsets[start_index] is None or offsets[end_index] is None:\n",
    "            continue\n",
    "\n",
    "          # Check 2: Skip answers that are negative length or above max length\n",
    "          if (\n",
    "            end_index < start_index\n",
    "            or end_index - start_index + 1 > MAX_ANS_LEN\n",
    "          ):\n",
    "            continue\n",
    "          \n",
    "          answers.append(\n",
    "            {\n",
    "              \"text\": context[offsets[start_index][0]:offsets[end_index][1]],\n",
    "              \"logit_score\": start_logit[start_index] + end_logit[end_index]\n",
    "            }\n",
    "          )\n",
    "    \n",
    "    if len(answers) > 0:\n",
    "      best_answer = max(answers, key=lambda x: x['logit_score'])\n",
    "      predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})\n",
    "    else:\n",
    "      predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "  \n",
    "  theoretical_answers = [\n",
    "    {\n",
    "      \"id\": ex['id'],\n",
    "      \"answers\": ex['answers']\n",
    "    } for ex in examples\n",
    "  ]\n",
    "\n",
    "  return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "\n",
    "compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop begins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "args = TrainingArguments(\n",
    "  \"bert-finetuned-squad\",\n",
    "  evaluation_strategy=\"no\",\n",
    "  save_strategy=\"epoch\",\n",
    "  learning_rate=2e-5,\n",
    "  num_train_epochs=3,\n",
    "  weight_decay=0.01,\n",
    "  fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=args,\n",
    "  train_dataset=train_dataset,\n",
    "  eval_dataset=validation_dataset,\n",
    "  tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation only happens at the end because of how the Trainer class is structured\n",
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import default_data_collator, get_scheduler\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format('torch')\n",
    "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "validation_set.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "  train_dataset,\n",
    "  shuffle=True,\n",
    "  collate_fn=default_data_collator,\n",
    "  batch_size=8\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "  validation_set,\n",
    "  collate_fn=default_data_collator,\n",
    "  batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "  model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "  'linear',\n",
    "  optimizer=optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "  # Training\n",
    "  model.train()\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    accelerator.backward(loss)\n",
    "\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    progress_bar.update(1)\n",
    "  \n",
    "  # Evaluation\n",
    "  model.eval()\n",
    "  start_logits = []\n",
    "  end_logits = []\n",
    "  accelerator.print(f\"Running evaluation on epoch: {epoch}\")\n",
    "  for batch in tqdm(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**batch)\n",
    "    \n",
    "    start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "    end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "\n",
    "  start_logits = np.concatenate(start_logits)\n",
    "  end_logits = np.concatenate(end_logits)\n",
    "  # Truncate to match the validation datasets length\n",
    "  start_logits = start_logits[: len(validation_dataset)]\n",
    "  end_logits = end_logits[: len(validation_dataset)]\n",
    "\n",
    "  metrics = compute_metrics(\n",
    "    start_logits, end_logits, validation_dataset, raw_datasets['validation']\n",
    "  )\n",
    "  print(f\"Epoch: {epoch}:\", metrics)\n",
    "\n",
    "  # # Save and upload\n",
    "  # accelerator.wait_for_everyone()\n",
    "  # unwrapped_model = accelerator.unwrap_model(model)\n",
    "  # unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "  # if accelerator.is_main_process:\n",
    "  #   tokenizer.save_pretrained(output_dir)\n",
    "  #   repo.push_to_hub(\n",
    "  #     commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "  #   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Inference\n",
    "question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n",
    "\n",
    "context = \"\"\"\n",
    " Transformers is backed by the three most popular deep learning libraries  Jax, PyTorch and TensorFlow  with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back  Transformers?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Data Collators\n",
    "\n",
    "Data collators collate lists of smaples into a single minibatch. They perform some additional preprocessing that make batches of matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collators - Pytorch\n",
    "data_collator = DefaultDataCollator() # return_tensors default to 'pt'\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=dataset,\n",
    "  data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Tensorflow\n",
    "data_collator = DefaultDataCollator(\n",
    "  return_tensors='tf'\n",
    ")\n",
    "train_set = dataset.to_tf_dataset(\n",
    "  columns=['input_ids', 'labels'],\n",
    "  shuffle=True,\n",
    "  batch_size=16,\n",
    "  collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data collators\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# 1. Basic padding - requires specific tokenizer for special padding token\n",
    "padding = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 2. Token classification & Seq2Seq - variable label length (both inputs need to be padded)\n",
    "DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    "\n",
    "# 3. Language modeling\n",
    "# Causal language modeling\n",
    "DataCollatorForLanguageModeling(\n",
    "  tokenizer=tokenizer,\n",
    "  mlm=False\n",
    ")\n",
    "# Masked language modeling\n",
    "DataCollatorForLanguageModeling(\n",
    "  tokenizer=tokenizer,\n",
    "  mlm=True,\n",
    "  mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Last updated: 2025-12-30T18:04:06.937522+08:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.12\n",
      "IPython version      : 8.37.0\n",
      "\n",
      "Compiler    : GCC 11.4.0\n",
      "OS          : Linux\n",
      "Release     : 6.6.87.2-microsoft-standard-WSL2\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 20\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_ubun_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
