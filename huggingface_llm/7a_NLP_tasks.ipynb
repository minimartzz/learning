{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tasks (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 18/12/2025   | Martin | Create  | Notebook created for various NLP tasks using HF | \n",
    "| 22/12/2025   | Martin | Update  | Continued translation. Up to before training model | \n",
    "| 23/12/2025   | Martin | Update  | Completed translation. Started on summarisation task | \n",
    "| 30/12/2025   | Martin | Update  | Completed summarisation task | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [1. Translation](#1-translation)\n",
    "* [2. Summarisation](#2-summarisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Tackle common NLP problems using LLMs built using the HF package:\n",
    "\n",
    "1. Translation\n",
    "2. Summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Translation\n",
    "\n",
    "- Seq-2-Seq task\n",
    "- Finetune existing language model (mT5, mBART, Marian - here)\n",
    "\n",
    "<u>Components</u>\n",
    "\n",
    "- Marian: English to French translation model\n",
    "- KDE4 dataset: Localised files for KDE (Apps for Linux desktops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 20:34:59.781888: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-08 20:35:00.994202: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-08 20:35:28.192329: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "  pipeline,\n",
    "  get_scheduler,\n",
    "  AutoTokenizer,\n",
    "  AutoModelForSeq2SeqLM,\n",
    "  DataCollatorForSeq2Seq,\n",
    "  Seq2SeqTrainingArguments,\n",
    "  Seq2SeqTrainer,\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEED = 20\n",
    "MAXLEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 139666\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"zh_CN\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 125699\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 13967\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets = raw_datasets['train'].train_test_split(train_size=0.9, seed=SEED)\n",
    "split_datasets['validation'] = split_datasets.pop('test')\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains 2 statements one for each language. The KDE4 dataset translates many of the technical terms to the corresponding language, but the pretrained model does not do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Installation prefix for Qt', 'zh_CN': 'Qt ÁöÑÂÆâË£ÖÂâçÁºÄ'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KDE4 dataset\n",
    "split_datasets['train'][1]['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Â∞ÜÂΩìÂâçÂÖâÊ†á‰ΩçÁΩÆ‰∏äÁöÑÂâ™Ë¥¥ÊùøÂÜÖÂÆπÁ≤òË¥¥Âà∞ÁºñËæëÂ≠óÊÆµ‰∏≠„ÄÇ'}]\n"
     ]
    }
   ],
   "source": [
    "# Pretrained model\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "print(translator(\"Pastes the clipboard contents at the current cursor position into the edit field.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pretrained components\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors='pt')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Installation prefix for Qt', 'zh_CN': 'Qt ÁöÑÂÆâË£ÖÂâçÁºÄ'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets['train'][1]['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [54596, 2765, 594, 10110, 15, 8, 632, 60, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [8, 632, 60, 8, 12, 9613, 637, 56891, 0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of splitting dataset and passing through tokenizer\n",
    "en_sentence = split_datasets['train'][1]['translation']['en']\n",
    "cn_sentence = split_datasets['train'][1]['translation']['zh_CN']\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target=cn_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "def preprocess(examples):\n",
    "  inputs = [ex['en'] for ex in examples['translation']]\n",
    "  targets = [ex['zh_CN'] for ex in examples['translation']]\n",
    "  model_inputs = tokenizer(inputs, text_target=targets, max_length=MAXLEN, truncation=True)\n",
    "\n",
    "  return model_inputs\n",
    "\n",
    "tokenized_dataset = split_datasets.map(\n",
    "  preprocess,\n",
    "  batched=True,\n",
    "  remove_columns=split_datasets['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-100` represents the padding values that should not be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': tensor([[54596,  2765,   594, 10110,    15,     8,   632,    60,     0],\n",
       "        [  457,     0, 65000, 65000, 65000, 65000, 65000, 65000, 65000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[    8,   632,    60,     8,    12,  9613,   637, 56891,     0],\n",
       "        [    8, 46315,     0,  -100,  -100,  -100,  -100,  -100,  -100]]), 'decoder_input_ids': tensor([[65000,     8,   632,    60,     8,    12,  9613,   637, 56891],\n",
       "        [65000,     8, 46315,     0, 65000, 65000, 65000, 65000, 65000]])})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_dataset['train'][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Training Details</u>\n",
    "\n",
    "- Model uses the `decoder_input_ids` with an attention mask to ensure that none of the after tokens are used during prediction\n",
    "- `generate()` is used to generate tokens one by one\n",
    "  - Need to set `predict_with_generate=True`\n",
    "- _BLEU score:_ Evaluates how close generations are to the expected message, penalising for repeated words\n",
    "- For translation tasks: Several sentences are used as labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 46.750469682990165,\n",
       " 'counts': [11, 6, 4, 3],\n",
       " 'totals': [12, 11, 10, 9],\n",
       " 'precisions': [91.66666666666667,\n",
       "  54.54545454545455,\n",
       "  40.0,\n",
       "  33.333333333333336],\n",
       " 'bp': 0.9200444146293233,\n",
       " 'sys_len': 12,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of good translation\n",
    "predictions = [\n",
    "  \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "references = [\n",
    "  [\n",
    "    \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "  ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 1.683602693167689,\n",
       " 'counts': [1, 0, 0, 0],\n",
       " 'totals': [4, 3, 2, 1],\n",
       " 'precisions': [25.0, 16.666666666666668, 12.5, 12.5],\n",
       " 'bp': 0.10539922456186433,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of poor translation\n",
    "predictions = [\"This This This This\"]\n",
    "references = [\n",
    "  [\n",
    "    \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "  ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "  preds, labels = eval_preds\n",
    "\n",
    "  # If the model returns more than the prediction logits\n",
    "  if isinstance(preds, tuple):\n",
    "    preds = preds[0]\n",
    "  \n",
    "  decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "  # Replace -100s in labels\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "  # Simple post-processing\n",
    "  decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "  decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "  result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "  return {'bleu': result['score']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2547/2852970220.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='438' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [219/219 29:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.4871249198913574,\n",
       " 'eval_model_preparation_time': 0.0017,\n",
       " 'eval_bleu': 28.04402379160332,\n",
       " 'eval_runtime': 638.9095,\n",
       " 'eval_samples_per_second': 21.861,\n",
       " 'eval_steps_per_second': 0.343}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "args = Seq2SeqTrainingArguments(\n",
    "  \"marian-finetuned-kd4e-en-to-ch_ZN\",\n",
    "  eval_strategy=\"no\",\n",
    "  save_strategy=\"epoch\",\n",
    "  learning_rate=2e-5,\n",
    "  per_device_train_batch_size=32,\n",
    "  per_device_eval_batch_size=64,\n",
    "  weight_decay=0.01,\n",
    "  save_total_limit=3,\n",
    "  num_train_epochs=3,\n",
    "  predict_with_generate=True,\n",
    "  fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "  model,\n",
    "  args,\n",
    "  train_dataset=tokenized_dataset['train'],\n",
    "  eval_dataset=tokenized_dataset['validation'],\n",
    "  data_collator=data_collator,\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate(max_length=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11787' max='11787' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11787/11787 12:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.558700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.222100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.143100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.994200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.926900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.898100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.926200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.905300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.903900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.897700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.902700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Software/venv/py310_ubun_venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9346717596054077,\n",
       " 'eval_model_preparation_time': 0.0017,\n",
       " 'eval_bleu': 41.547050185618154,\n",
       " 'eval_runtime': 264.7145,\n",
       " 'eval_samples_per_second': 52.763,\n",
       " 'eval_steps_per_second': 0.827,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the training loop\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate(max_length=MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model improved from BLEU score of 28.04 -> 41.55 which is a pretty good result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pytorch training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "  tokenized_dataset['train'],\n",
    "  shuffle=True,\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=8\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "  tokenized_dataset['validation'],\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "  model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "  \"linear\",\n",
    "  optimizer=optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "  predictions = predictions.cpu().numpy()\n",
    "  labels = labels.cpu().numpy()\n",
    "\n",
    "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "  # Replace -100 in the labels as we can't decode them.\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "  # Some simple post-processing\n",
    "  decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "  decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "  return decoded_preds, decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "  model.train()\n",
    "  for batch in train_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    accelerator.backward(loss)\n",
    "\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    progress_bar.update(1)\n",
    "  \n",
    "  # Evaluation at each epoch\n",
    "  model.eval()\n",
    "  for batch in tqdm(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "      generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "        batch['input_ids'],\n",
    "        attention_mask=batch['attention_mask'],\n",
    "        max_length=128\n",
    "      )\n",
    "    labels = batch['labels']\n",
    "\n",
    "    # Pad predictions and labels before gathering\n",
    "    generated_tokens = accelerator.pad_across_process(\n",
    "      generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_process(\n",
    "      labels, dim=1, pad_index=-100\n",
    "    )\n",
    "\n",
    "    # Gather predictions\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predcitions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "  \n",
    "results = metric.compute()\n",
    "print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the model\n",
    "# accelerator.wait_for_everyone()\n",
    "# unwrapped_model = accelerator.unwrap_model(model)\n",
    "# unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "# if accelerator.is_main_process:\n",
    "#   tokenizer.save_pretrained(output_dir)\n",
    "#   repo.push_to_hub(\n",
    "#     commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer_test = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "translator_test = pipeline('translation', model=model, tokenizer=tokenizer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'ÈªòËÆ§Âà∞Êâ©Â±ïÁ∫øÊù°'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator_test(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Summarisation\n",
    "\n",
    "Challenging NLP task that requires the model to understand long passages and generate coherent text that capture the main topic\n",
    "\n",
    "- ü§ñ Output: Bilingual English & Spanish model that summarises customer reviews\n",
    "- üíæ Dataset: Multilingual Amazon Reviews Corpus\n",
    "  - Use the title as the target summaries\n",
    "\n",
    "‚úíÔ∏è NOTE: Dataset doesn't exist on HF, so take from: https://www.kaggle.com/datasets/mexwell/amazon-reviews-multi/data?select=train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 17:40:15.887955: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-30 17:40:16.802300: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-30 17:40:28.220628: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import evaluate\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "from huggingface_hub import notebook_login\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f12f9722fa14560adc503bc500033ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0', 'review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"train\": \"train.csv\", \"validation\": \"validation.csv\", \"test\": \"test.csv\"}\n",
    "total_dataset = load_dataset(\"./data/amazon_review_multi\", data_files=data_files)\n",
    "\n",
    "eng_dataset = total_dataset.filter(lambda x: x['language'] == 'en')\n",
    "spa_dataset = total_dataset.filter(lambda x: x['language'] == 'es')\n",
    "eng_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- Target: Title (summary) | Data: Review body\n",
    "- Selecting only book and ebook reviews\n",
    "- Remove examples with short titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Title: Worked in front position, not rear\n",
      ">> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.\n",
      "\n",
      ">> Title: meh\n",
      ">> Review: Does it‚Äôs job and it‚Äôs gorgeous but mine is falling apart, I had to basically put it together again with hot glue\n",
      "\n",
      ">> Title: Can't beat these for the money\n",
      ">> Review: Bought this for handling miscellaneous aircraft parts and hanger \"stuff\" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn't get brittle and split like my older plastic drawers did. I like the all-plastic construction. It's heavy duty enough to hold metal parts, but being made of plastic it's not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can't beat it. Best one of these I've bought to date-- and I've been using some version of these for over forty years.\n"
     ]
    }
   ],
   "source": [
    "def show_samples(\n",
    "  dataset: datasets.dataset_dict.DatasetDict, \n",
    "  num_samples: int=3,\n",
    "  seed: int=42\n",
    "):\n",
    "  sample = dataset['train'].shuffle(seed=seed).select(range(num_samples))\n",
    "  for example in sample:\n",
    "    print(f\"\\n>> Title: {example['review_title']}\")\n",
    "    print(f\">> Review: {example['review_body']}\")\n",
    "\n",
    "show_samples(eng_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Title: I'm dissapointed.\n",
      ">> Review: I guess I had higher expectations for this book from the reviews. I really thought I'd at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I'm dissapointed.\n",
      "\n",
      ">> Title: Good art, good price, poor design\n",
      ">> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it's less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar\n",
      "\n",
      ">> Title: Helpful\n",
      ">> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.\n"
     ]
    }
   ],
   "source": [
    "# Additional filtering on books and ebooks to reduce dataset size\n",
    "def filter_books(example: dict):\n",
    "  return (\n",
    "    example['product_category'] == \"book\" or\n",
    "    example['product_category'] == \"digital_ebook_purchase\"\n",
    "  )\n",
    "\n",
    "eng_books = eng_dataset.filter(filter_books)\n",
    "spa_books = spa_dataset.filter(filter_books)\n",
    "show_samples(eng_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Title: Easy to follow!!!!\n",
      ">> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.\n",
      "\n",
      ">> Title: PARCIALMENTE DA√ëADO\n",
      ">> Review: Me lleg√≥ el d√≠a que tocaba, junto a otros libros que ped√≠, pero la caja lleg√≥ en mal estado lo cual da√±√≥ las esquinas de los libros porque ven√≠an sin protecci√≥n (forro).\n",
      "\n",
      ">> Title: no lo he podido descargar\n",
      ">> Review: igual que el anterior\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the english and spanish datasets\n",
    "books_dataset = DatasetDict()\n",
    "\n",
    "for split in eng_books.keys():\n",
    "  books_dataset[split] = concatenate_datasets(\n",
    "    [eng_books[split], spa_books[split]]\n",
    "  )\n",
    "  books_dataset[split] = books_dataset[split].shuffle(seed=42)\n",
    "\n",
    "show_samples(books_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 9672\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0', 'review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 238\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 245\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove titles with 1-2 words (heuristic: split on whitespace)\n",
    "books_dataset = books_dataset.filter(lambda x: len(x['review_title'].split(\" \")) > 2)\n",
    "books_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `mT5` model which is a multi-lingual version of the `T5` model.\n",
    "\n",
    "- `T5` uses prefixes to determine which task the text-2-text transformer performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/mnt/d/Software/venv/py310_ubun_venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"google/mt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 309, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = tokenizer(\"I loved reading the Hunger Games!\")\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚ñÅI', '‚ñÅ', 'loved', '‚ñÅreading', '‚ñÅthe', '‚ñÅHung', 'er', '‚ñÅGames', '!', '</s>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(sample.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bd1cf5f4de47e7ba6e78953b108798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_max_len = 512\n",
    "summ_max_len = 30\n",
    "\n",
    "def preprocess_function(example):\n",
    "  tok_inp = tokenizer(\n",
    "    example['review_body'],\n",
    "    max_length=input_max_len,\n",
    "    truncation=True\n",
    "  )\n",
    "  sum_inp = tokenizer(\n",
    "    example['review_title'],\n",
    "    max_length=summ_max_len,\n",
    "    truncation=True\n",
    "  )\n",
    "  tok_inp['labels'] = sum_inp['input_ids']\n",
    "  return tok_inp\n",
    "\n",
    "tokenized_dataset = books_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE metric\n",
    "\n",
    "The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric is used to measure the performance of models on text summarisation tasks. It comapres a generated summary to one or more reference summaries.\n",
    "\n",
    "- ROUGE-N: Compares n-grams of the generation with n-grams of references\n",
    "  - Larger N values suffer when sentences are longer\n",
    "- ROUGE-L: Uses the longest common subsequence (LCS) to compute subsequent metrics\n",
    "  - Captures sentence structure more accurately than n-gram version\n",
    "- Recall, precision and F1 for each \"ROUGE\" is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(0.923076923076923),\n",
       " 'rouge2': np.float64(0.7272727272727272),\n",
       " 'rougeL': np.float64(0.923076923076923),\n",
       " 'rougeLsum': np.float64(0.923076923076923)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_score = evaluate.load('rouge')\n",
    "\n",
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\"\n",
    "\n",
    "scores = rouge_score.compute(\n",
    "  predictions=[generated_summary], references=[reference_summary]\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a baseline model: Lead-3 baseline - use the first 3 sentences as the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(0.16841976040654966),\n",
       " 'rouge2': np.float64(0.08823818593467445),\n",
       " 'rougeL': np.float64(0.15569693876358195),\n",
       " 'rougeLsum': np.float64(0.15985686139905012)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def first_three_sentences(text):\n",
    "  return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "\n",
    "def evaluate_baseline(dataset, metric):\n",
    "  summaries = [first_three_sentences(text) for text in dataset['review_body']]\n",
    "  return metric.compute(predictions=summaries, references=dataset['review_title'])\n",
    "\n",
    "scores = evaluate_baseline(books_dataset['validation'], rouge_score)\n",
    "scores # Scores are in %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_TRAIN_EPOCHS = 8\n",
    "LOGGING_STEPS = len(tokenized_dataset['train']) // BATCH_SIZE\n",
    "MODEL_NAME = model_checkpoint.split('/')[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "  output_dir=f\"{MODEL_NAME}-finetuned-amazon-en-es\",\n",
    "  eval_strategy=\"epoch\",\n",
    "  learning_rate=5.6e-5,\n",
    "  per_device_train_batch_size=BATCH_SIZE,\n",
    "  per_device_eval_batch_size=BATCH_SIZE,\n",
    "  weight_decay=0.01,\n",
    "  save_strategy=\"no\",\n",
    "  num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "  predict_with_generate=True,\n",
    "  logging_steps=LOGGING_STEPS,\n",
    "  push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "  predictions, labels = eval_pred\n",
    "  # Check the datatype of the prediction\n",
    "  if isinstance(predictions, tuple):\n",
    "    predictions = predictions[0]\n",
    "  if np.issubdtype(predictions.dtype, np.floating):\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "  \n",
    "  predictions = np.nan_to_num(predictions, nan=0, posinf=0, neginf=0)\n",
    "  predictions = predictions.clip(min=0).astype(np.int32)\n",
    "  \n",
    "  # Decode predictions\n",
    "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "  # Decode labels\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "  # ROUGE requires newline after each sentence\n",
    "  cleaned_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "  cleaned_labels = [\"\\n\".join(sent_tokenize(lab.strip())) for lab in decoded_labels]\n",
    "\n",
    "  # Compute scores\n",
    "  scores = rouge_score.compute(predictions=cleaned_preds, references=cleaned_labels, use_stemmer=True)\n",
    "\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   653,   1957,   1314,    261,   2757,   1280,    435,    259,  29166,\n",
       "            263,    269,    774,   5547,      1,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0],\n",
       "        [   336,   6998,    481,   1150,  11807,   1590,  16339,    360,    261,\n",
       "            305,   3127,   2780,    261,    336,  35042,    345,    261,   3141,\n",
       "         136052,    285,    259,    266,   1425,    313,   2983, 106419,    272,\n",
       "            311,   4065,    260,  36874,    261,   1909,    259,    262,  22163,\n",
       "            639,    259,   7505,    332,   9066,  88398,    265,   5105,   6320,\n",
       "           4906,    261,    336,  21847,    345,   1590,  16339,    360,    260,\n",
       "           4630,   5897,    320, 146261,    260,    486,  21101,   1998,   3684,\n",
       "           2606,   2316,    609,    639,   3014,  11665,    416,    260,    336,\n",
       "            639,   7779,    259,    266,   1425,  57512,   9066,   5401,    260,\n",
       "            336,    259,  91451,    259,    262,  27613,    332,    259,  34779,\n",
       "            260,   1494,    639,    259,    262,   3005,    584,  62351,    288,\n",
       "            461,   6801,  22590,  62799,   3093,    259,    262,  20233,   3622,\n",
       "            304,    259,  11994,    259,  36260,  48971,    533,    418,   2725,\n",
       "            484,  27067,   1059,    260,  18646,    521,    259,   3659,   1063,\n",
       "           1388,    514,   3622,   3004,  26222,    260,    298, 109508,    276,\n",
       "            533,   1425,    288,    390,    259, 131690,    260,    336,    259,\n",
       "          25505,    609,    288,    390,    287,   3435,    336,   4906,   6338,\n",
       "            259,   6924,    259,  29426,    261,   2469,    351,    259, 109933,\n",
       "            260,    563,  15129,    261,    336,    277,    857,  36975,    345,\n",
       "          14355,    305,    259,    265, 111962,    288,  27039,    714,    260,\n",
       "            459,  25640,    259,    262,   5150,   4906,    260,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  298,   259,  5994,   269,   774,  5547,     1],\n",
       "        [  298, 10380,   304, 13992,   291,     1,  -100]]), 'decoder_input_ids': tensor([[    0,   298,   259,  5994,   269,   774,  5547],\n",
       "        [    0,   298, 10380,   304, 13992,   291,     1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(books_dataset['train'].column_names)\n",
    "features = [tokenized_dataset['train'][i] for i in range(2)]\n",
    "data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45710/46878106.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9672' max='9672' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9672/9672 17:05, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.667700</td>\n",
       "      <td>3.230450</td>\n",
       "      <td>0.165955</td>\n",
       "      <td>0.085627</td>\n",
       "      <td>0.159895</td>\n",
       "      <td>0.157942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.657100</td>\n",
       "      <td>3.128719</td>\n",
       "      <td>0.176765</td>\n",
       "      <td>0.092918</td>\n",
       "      <td>0.172067</td>\n",
       "      <td>0.170659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.443900</td>\n",
       "      <td>3.070215</td>\n",
       "      <td>0.172308</td>\n",
       "      <td>0.087527</td>\n",
       "      <td>0.168807</td>\n",
       "      <td>0.166907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.318700</td>\n",
       "      <td>3.064163</td>\n",
       "      <td>0.162202</td>\n",
       "      <td>0.076649</td>\n",
       "      <td>0.161197</td>\n",
       "      <td>0.159230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.233500</td>\n",
       "      <td>3.054373</td>\n",
       "      <td>0.169902</td>\n",
       "      <td>0.079787</td>\n",
       "      <td>0.167870</td>\n",
       "      <td>0.166542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.168900</td>\n",
       "      <td>3.042906</td>\n",
       "      <td>0.161637</td>\n",
       "      <td>0.069749</td>\n",
       "      <td>0.158666</td>\n",
       "      <td>0.156296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.135600</td>\n",
       "      <td>3.027733</td>\n",
       "      <td>0.158830</td>\n",
       "      <td>0.069478</td>\n",
       "      <td>0.155854</td>\n",
       "      <td>0.153870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.101200</td>\n",
       "      <td>3.032171</td>\n",
       "      <td>0.161145</td>\n",
       "      <td>0.071819</td>\n",
       "      <td>0.158743</td>\n",
       "      <td>0.156731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9672, training_loss=3.3408081710979505, metrics={'train_runtime': 1025.4237, 'train_samples_per_second': 75.458, 'train_steps_per_second': 9.432, 'total_flos': 1.187112454422528e+16, 'train_loss': 3.3408081710979505, 'epoch': 8.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "  model,\n",
    "  args,\n",
    "  train_dataset=tokenized_dataset['train'],\n",
    "  eval_dataset=tokenized_dataset['validation'],\n",
    "  data_collator=data_collator,\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.032170534133911,\n",
       " 'eval_rouge1': 0.16114484674501287,\n",
       " 'eval_rouge2': 0.07181944491841204,\n",
       " 'eval_rougeL': 0.15874269093833066,\n",
       " 'eval_rougeLsum': 0.15673079440603171,\n",
       " 'eval_runtime': 7.1888,\n",
       " 'eval_samples_per_second': 33.107,\n",
       " 'eval_steps_per_second': 4.173,\n",
       " 'epoch': 8.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ed14d319c14bae811cd465e117ac79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0270ab79f1b947ebabb114f365a974b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Minimartzz/mt5-small-finetuned-amazon-en-es/commit/a3af9b8e041e3885a45f7f171acfe70f8bc29207', commit_message='Training complete', commit_description='', oid='a3af9b8e041e3885a45f7f171acfe70f8bc29207', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Minimartzz/mt5-small-finetuned-amazon-en-es', endpoint='https://huggingface.co', repo_type='model', repo_id='Minimartzz/mt5-small-finetuned-amazon-en-es'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\", tags=\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e871db3320243ce9030f7a4f1de5e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/757 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef3a14aee544cb0bf61c6a9ce9e71ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7dcc9b0460445abf88876f46713c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66f2bd30d8c48e59909bf41c17793af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e11ed2d1a5e42b4a8ca7cc8a2f2e147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9739cc8ccfb4a478ec2da0413aba60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e7a838abde483f8724d4c354a73129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/416 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "hub_model_id = \"Minimartzz/mt5-small-finetuned-amazon-en-es\"\n",
    "summarizer = pipeline(\"summarization\", model=hub_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 20, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Review: Era lo que nos ped√≠an en el instituto'\n",
      "\n",
      "'>>> Title: Ha llegado muy a tiempo'\n",
      "\n",
      "'>>> Summary: No me ha gustado'\n"
     ]
    }
   ],
   "source": [
    "def print_summary(idx):\n",
    "  review = books_dataset[\"test\"][idx][\"review_body\"]\n",
    "  title = books_dataset[\"test\"][idx][\"review_title\"]\n",
    "  summary = summarizer(books_dataset[\"test\"][idx][\"review_body\"])[0][\"summary_text\"]\n",
    "  print(f\"'>>> Review: {review}'\")\n",
    "  print(f\"\\n'>>> Title: {title}'\")\n",
    "  print(f\"\\n'>>> Summary: {summary}'\")\n",
    "\n",
    "print_summary(110) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from huggingface_hub import get_full_repo_name, Repository\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataloader = DataLoader(\n",
    "  tokenized_dataset['train'],\n",
    "  shuffle=True,\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "  tokenized_dataset['validation'],\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "  model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "num_train_epochs = 10\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "  \"linear\",\n",
    "  optimizer=optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  labels = [label.strip() for label in labels]\n",
    "\n",
    "  preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "  labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "  return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving\n",
    "model_name = \"mt5-finetuned-amazon-en-es-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "\n",
    "# output_dir = \"results-mt5-finetuned-squad-accelerate\"\n",
    "# repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_training_steps):\n",
    "  model.train()\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(**batch)\n",
    "    loss = out.loss\n",
    "    accelerator.backward(loss)\n",
    "\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    progress_bar.update(1)\n",
    "  \n",
    "  model.eval()\n",
    "  for step, batch in enumerate(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "      generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "        batch['input_ids'],\n",
    "        attention_mask=batch['attention_mask']\n",
    "      )\n",
    "      generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens,\n",
    "        dim=1,\n",
    "        pad_index=tokenizer.pad_token_id\n",
    "      )\n",
    "\n",
    "      labels = batch['labels']\n",
    "      labels = accelerator.pad_across_processes(\n",
    "        batch['labels'],\n",
    "        dim=1,\n",
    "        pad_index=tokenizer.pad_token_id\n",
    "      )\n",
    "\n",
    "      generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "      labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "      labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "      if isinstance(generated_tokens, tuple):\n",
    "        generated_tokens = generated_tokens[0]\n",
    "      decoded_preds = tokenizer.batch_decode(\n",
    "        generated_tokens, skip_special_tokens=True\n",
    "      )\n",
    "      decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "      decoded_preds, decoded_labels = postprocess_text(\n",
    "        decoded_preds, decoded_labels\n",
    "      )\n",
    "\n",
    "      rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "  \n",
    "  result = rouge_score.compute()\n",
    "  print(f\"Epoch {epoch}:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2025-06-18T19:03:45.452311+08:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.9\n",
      "IPython version      : 8.31.0\n",
      "\n",
      "Compiler    : MSC v.1938 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel\n",
      "CPU cores   : 20\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_ubun_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
